{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import fitz\n",
    "import pandas as pd\n",
    "import os\n",
    "from langchain.docstore.document import Document\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib.concurrent import process_map  # or thread_map\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from multiprocessing import Manager\n",
    "from functools import partial\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/soma4/.kaggle/kaggle.json'\n",
      "Downloading arxiv.zip to /home/soma4/YEONDOO-arxiv-with-version/YEONDOO-arxiv\n",
      "100%|██████████████████████████████████████| 1.17G/1.17G [01:53<00:00, 11.1MB/s]\n",
      "100%|██████████████████████████████████████| 1.17G/1.17G [01:53<00:00, 11.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d Cornell-University/arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of dicts and convert into a pandas df.\n",
    "arxiv_data = []\n",
    "for line in open('../arxiv-metadata-oai-snapshot.json', 'r'):\n",
    "    arxiv_data.append(json.loads(line))\n",
    "df = pd.DataFrame.from_records(arxiv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df['categories'].str.contains('cs', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3264255406434721"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_filtered)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "749538"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del arxiv_data\n",
    "gc.collect()\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir=\"/home/soma4/YEONDOO-arxiv-with-version/YEONDOO-arxiv/data/\"\n",
    "year=\"14\"\n",
    "month=\"01\"\n",
    "path_data=os.path.join(basedir,year,month)\n",
    "id_list=os.listdir(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list=[id.split('v')[0] for id in os.listdir(path_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1401.4606'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cs.AI', 'cs.LO']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['id']==id_list[2]]['categories'].item().split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>versions</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>493844</th>\n",
       "      <td>1401.4613</td>\n",
       "      <td>Peter Jeavons</td>\n",
       "      <td>Peter Jeavons, Justyna Petke</td>\n",
       "      <td>Local Consistency and SAT-Solvers</td>\n",
       "      <td>None</td>\n",
       "      <td>Journal Of Artificial Intelligence Research, V...</td>\n",
       "      <td>10.1613/jair.3531</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI cs.LO</td>\n",
       "      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n",
       "      <td>Local consistency techniques such as k-consi...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 18 Jan 201...</td>\n",
       "      <td>2014-01-21</td>\n",
       "      <td>[[Jeavons, Peter, ], [Petke, Justyna, ]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id      submitter                       authors  \\\n",
       "493844  1401.4613  Peter Jeavons  Peter Jeavons, Justyna Petke   \n",
       "\n",
       "                                    title comments  \\\n",
       "493844  Local Consistency and SAT-Solvers     None   \n",
       "\n",
       "                                              journal-ref                doi  \\\n",
       "493844  Journal Of Artificial Intelligence Research, V...  10.1613/jair.3531   \n",
       "\n",
       "       report-no   categories  \\\n",
       "493844      None  cs.AI cs.LO   \n",
       "\n",
       "                                                  license  \\\n",
       "493844  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
       "\n",
       "                                                 abstract  \\\n",
       "493844    Local consistency techniques such as k-consi...   \n",
       "\n",
       "                                                 versions update_date  \\\n",
       "493844  [{'version': 'v1', 'created': 'Sat, 18 Jan 201...  2014-01-21   \n",
       "\n",
       "                                  authors_parsed  \n",
       "493844  [[Jeavons, Peter, ], [Petke, Justyna, ]]  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['id']==id_list[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "id=\"1401.0001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493837    2014-01-21\n",
       "Name: update_date, dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['id']==id]['update_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "papper_id=id\n",
    "published=df[df['id']==id]['update_date']\n",
    "title=df[df['id']==id].title.item()\n",
    "\n",
    "authors_replaced=df[df['id']==id].authors.item().replace('and',',').split(',')\n",
    "authors=authors_replaced.split(',')\n",
    "summary=df[df['id']==id].abstract.item()\n",
    "url=\"http://arxiv.org/abs/\"+id\n",
    "ref=df[df['id']==id]['journal-ref'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nils Bertschinger ',\n",
       " ' David H. Wolpert ',\n",
       " ' Eckehard Olbrich ',\n",
       " '\\n  Juergen Jost']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['id']==id].authors.item().replace('and',',').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nils Bertschinger and David H. Wolpert and Eckehard Olbrich and\\n  Juergen Jost'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['id']==id].authors.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014-01-21'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "published=df[df['id']==id]['update_date'].item()\n",
    "published\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1401.4606'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_list[0][:-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Wrapper(shared_list,doc_file_name):\n",
    "    \n",
    "    id=doc_file_name.split('v')[0]\n",
    "    file_path=os.path.join(basedir,year,month,doc_file_name)\n",
    "    try:\n",
    "        with fitz.open(file_path) as doc_file:\n",
    "            text: str = \"\".join(page.get_text() for page in doc_file)\n",
    "    except:\n",
    "        print(file_path)\n",
    "        return\n",
    "    result=df_filtered[df_filtered[\"id\"]==id]\n",
    "\n",
    "    metadata = {\n",
    "        \"Published\": str(result[\"update_date\"].item()),\n",
    "        \"Title\": result.title.item(),\n",
    "        \"Authors\": result.authors.item().replace('and',',').split(','),\n",
    "        \"Summary\": result.abstract.item(),\n",
    "\n",
    "        \"paper_id\": id,\n",
    "\n",
    "        \"journal_ref\": result['journal-ref'].item(),\n",
    "        \n",
    "        \"categories\": result.categories,\n",
    "        \"source\": \"http://arxiv.org/abs/\"+id,\n",
    "    }\n",
    "    doc = Document(\n",
    "        page_content=text, metadata=metadata\n",
    "    )\n",
    "    shared_list.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Wrapper2(shared_list,inputs):\n",
    "    \n",
    "    doc_file_name = inputs[0]\n",
    "    result=inputs[1]\n",
    "    id=doc_file_name.split('v')[0]\n",
    "    file_path=os.path.join(basedir,year,month,doc_file_name)\n",
    "    with fitz.open(file_path) as doc_file:\n",
    "        text: str = \"\".join(page.get_text() for page in doc_file)\n",
    "    \n",
    "    # result=df[df[\"id\"]==id]\n",
    "\n",
    "    metadata = {\n",
    "        \"Published\": str(result[\"update_date\"].item()),\n",
    "        \"Title\": result.title.item(),\n",
    "        \"Authors\": result.authors.item().replace('and',',').split(','),\n",
    "        \"Summary\": result.abstract.item(),\n",
    "\n",
    "        \"paper_id\": id,\n",
    "\n",
    "        \"journal_ref\": result['journal-ref'].item(),\n",
    "        \n",
    "        \"categories\": result.categories,\n",
    "        \"source\": \"http://arxiv.org/abs/\"+id,\n",
    "    }\n",
    "    doc = Document(\n",
    "        page_content=text, metadata=metadata\n",
    "    )\n",
    "    shared_list.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir=\"/home/soma4/YEONDOO-arxiv-with-version/YEONDOO-arxiv/data/\"\n",
    "year=\"14\"\n",
    "month=\"01\"\n",
    "path_data=os.path.join(basedir,year,month)\n",
    "pdf_list=os.listdir(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202/202 [00:16<00:00, 12.24it/s]\n"
     ]
    }
   ],
   "source": [
    "inputs=[]\n",
    "for pdf_id in tqdm(pdf_list):\n",
    "    tmp=[]\n",
    "    tmp.append(pdf_id)\n",
    "    id=pdf_id.split('v')[0]\n",
    "    tmp.append(df_filtered[df_filtered[\"id\"]==id])\n",
    "    inputs.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665302d9c11c459daaa55a0842db14c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/soma4/YEONDOO-arxiv-with-version/YEONDOO-arxiv/data/14/01/filelist.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chunksize=2\n",
    "\n",
    "manager = Manager()\n",
    "shared_list = manager.list()\n",
    "process_map(partial(Wrapper, shared_list),pdf_list,max_workers=5,chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Journal of Artiﬁcial Intelligence Research 42 (2011) 607-659\\nSubmitted 09/11; published 12/11\\nDrake: An Eﬃcient Executive for Temporal\\nPlans with Choice\\nPatrick R. Conrad\\nprconrad@mit.edu\\nBrian C. Williams\\nwilliams@mit.edu\\nRoom 32-227\\n32 Vassar St\\nCambridge, MA 02139 USA\\nAbstract\\nThis work presents Drake, a dynamic executive for temporal plans with choice. Dy-\\nnamic plan execution strategies allow an autonomous agent to react quickly to unfolding\\nevents, improving the robustness of the agent. Prior work developed methods for dynami-\\ncally dispatching Simple Temporal Networks, and further research enriched the expressive-\\nness of the plans executives could handle, including discrete choices, which are the focus of\\nthis work. However, in some approaches to date, these additional choices induce signiﬁcant\\nstorage or latency requirements to make ﬂexible execution possible.\\nDrake is designed to leverage the low latency made possible by a preprocessing step\\ncalled compilation, while avoiding high memory costs through a compact representation.\\nWe leverage the concepts of labels and environments, taken from prior work in Assumption-\\nbased Truth Maintenance Systems (ATMS), to concisely record the implications of the\\ndiscrete choices, exploiting the structure of the plan to avoid redundant reasoning or stor-\\nage. Our labeling and maintenance scheme, called the Labeled Value Set Maintenance\\nSystem, is distinguished by its focus on properties fundamental to temporal problems, and,\\nmore generally, weighted graph algorithms. In particular, the maintenance system focuses\\non maintaining a minimal representation of non-dominated constraints. We benchmark\\nDrake’s performance on random structured problems, and ﬁnd that Drake reduces the size\\nof the compiled representation by a factor of over 500 for large problems, while incurring\\nonly a modest increase in run-time latency, compared to prior work in compiled executives\\nfor temporal plans with discrete choices.\\n1. Introduction\\nModel-based executives elevate commanding of autonomous systems to the level of goal\\nstates while providing guarantees of correctness (Williams, Ingham, Chung, & Elliott, 2003).\\nUsing a model-based executive, a user can provide a speciﬁcation of the goal behavior of\\nthe robot and leave it to a program, the executive, to determine an appropriate course of\\naction that meets those goals. Temporal plan executives are designed to work with plans\\nincluding timing requirements.\\nTypically, for an executive to be robust to disturbances, it must be able to react to the\\noutcomes of events on the ﬂy, otherwise, even seemingly inconsequential variations in the\\noutcomes of events may cause a failure. Thus, it can be helpful to follow a strategy of least\\ncommitment and delay each decision until it is actually time to act on that decision, allowing\\nthe executive to act with as much information as possible. In the case of temporal plans,\\nan executive following this strategy is said to dynamically dispatch the plan (Muscettola,\\nc⃝2011 AI Access Foundation. All rights reserved.\\nConrad & Williams\\nMorris, & Tsamardinos, 1998). Such an executive is responsible for determining when to\\nschedule events as late as possible while guaranteeing that a consistent schedule exists for\\nall the remaining events. If an external disturbance causes some timing requirement to be\\nviolated, then the executive should discover the failure and signal it as soon as possible.\\nMaking such decisions on the ﬂy requires some care, as on-line temporal reasoning can\\nintroduce latency that is unacceptable for a real-time system. Therefore, Muscettola et al.\\n(1998) developed a low-latency executive for Simple Temporal Networks (STNs), where an\\nSTN is comprised of a set of events and diﬀerence constraints on the time of execution of\\nthe events. To achieve low latency, the executive is broken into two parts, a compiler and\\na dispatcher. The compiler is run in advance to discover and explicitly record all temporal\\nconstraints that cannot be quickly inferred on-line, thereby computing the dispatchable\\nform of the plan. The dispatcher uses this form to make real-time decisions using a greedy\\nstrategy and local, low-latency inferences.\\nWhile dynamic scheduling has proven eﬀective, robustness can further be improved by\\nmaking additional decision dynamically, such as the assignment of an activity to a partic-\\nular resource. Encoding these decisions requires a more expressive formalism than STNs.\\nConsequently, subsequent research has developed eﬃcient executives for more expressive\\nframeworks, many of which are variants of the STN. Examples of added features include\\nexplicit modeling of uncertainty (Morris, Muscettola, & Vidal, 2001; Rossi, Venable, &\\nYorke-Smith, 2006; Shah & Williams, 2008), discrete choices (Kim, Williams, & Abramson,\\n2001; Tsamardinos, Pollack, & Ganchev, 2001; Combi & Posenato, 2009; Shah & Williams,\\n2008), preferences (Hiatt, Zimmerman, Smith, & Simmons, 2009; Khatib, Morris, Morris,\\n& Rossi, 2001; Kim et al., 2001), discrete observations (Tsamardinos, Vidal, & Pollack,\\n2003), and combinations thereof.\\nThis work focuses on enriching the executive to simultaneously schedule events and\\nmake discrete choices as the execution unfolds. The ability to make discrete choices greatly\\nenriches an executive by oﬀering it the ability to dynamically allocate resources, order activ-\\nities, and choose between alternate methods (sub-plans) for achieving goals. Although prior\\nworks have developed executives for this type of plan, they have trade-oﬀs in performance.\\nFor example, Tsamardinos, Pollack, and Ganchev (2001) presented an executive for Dis-\\njunctive Temporal Networks (DTN), a variant of STNs that include discrete choices. Their\\nexecutive extends the compilation strategy for STNs by breaking the DTN into its complete\\nexponential set of component STNs and then compiling and dispatching each in parallel.\\nTheir strategy oﬀers low latency, but incurs a high storage cost for the dispatchable plan.\\nAnother example, Kirk, is an executive for Temporal Plan Networks (TPNs), which extends\\nSTNs by including a hierarchical choice between sub-plans, developed by Kim, Williams,\\nand Abramson (2001). Kirk selects a set of choices and performs incremental re-planning\\nwhenever a disturbance invalidates that choice, leaving a small memory footprint, but po-\\ntentially inducing high latency when it selects new choices. Chaski is an executive presented\\nby Shah and Williams (2007) for temporal plans with resource allocation, whose expres-\\nsiveness is between that of STNs and DTNs. Chaski takes an approach which is a hybrid\\nof the incremental strategy of Kirk and the compiled approach of Tsamardinos et al.: its\\ncompiled representation is a base plan and a set of incremental diﬀerences, which provides\\nthe beneﬁts of compiled execution while improving eﬃciency by exploiting structure in the\\nplan .\\n608\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nWe develop Drake, a novel executive for temporal plans with choice encoded using\\nthe expressive representation of DTNs (DTNs dominate TPNs, TCNs, and STNs). Drake\\nachieves low run-time latency through compilation, yet requires less storage than the fully\\nexponential expansion approach taken by Tsamardinos et al. (2001). In order to accom-\\nplish this, Drake works on a compact representation of the temporal constraints and discrete\\nchoices. To develop the compact representation, we begin with the idea, taken from truth\\nmaintenance, of labeling consequences of inferences with the minimal set of choices that im-\\nply the consequence; this minimal set is called an environment (McDermott, 1983; de Kleer,\\n1986). By monotonicity of inference, this consequence also holds for all sets of choices that\\nare a superset of the minimal environment, thus the environment is a compact encoding of\\nthe decision contexts in which that consequence holds.\\nThese ideas are directly applicable to temporal reasoning problems; Drake extends them\\nby leveraging properties fundamental to temporal reasoning problems, and weighted graph\\nproblems in general. More speciﬁcally, temporal reasoning is non-monotonic, in the sense\\nthat it does not need to explicitly represent all derivable constraints, only the tightest possi-\\nble ones, referred to as the non-dominated constraints. Drake uses this property throughout\\nto reduce the computations and storage required. For example, in the inequality A ≤ 4 ≤ 8,\\nwhere A is a temporal event, there is no need to store the constraint A ≤ 8, as the tighter\\ninequality makes it unnecessary, or dominates it. The focus on non-dominated values or\\nconstraints is central to a range of inference problems, including temporal inference, interval\\nreasoning, and inference over weighted graphs.\\nDechter, Meiri, and Pearl (1991) proved that STN inference problems are reducible to\\na widely used set of inference methods on weighted graphs, such as Single Source Shortest\\nPath and All-Pairs Shortest Path Problems. Our approach is to develop labeled analogues\\nto the weighted graph structures that support these shortest path algorithms, providing a\\ncompact representation for Drake. In this paper, we ﬁrst present a new formalism for plans\\nwith choice, the Labeled Simple Temporal Network (Labeled STN), which has the same\\nexpressiveness as previous formalisms, but which shares ideas with the rest of our techniques.\\nSecond, we explain a system for maintaining and deriving compact representations of values\\nthat vary with choices, called the Labeled Value Set Maintenance System. Then, we use\\nLabeled Value Sets to construct Labeled Distance Graphs, which are distance graphs where\\nthe edge weights may vary depending on the discrete choices. Finally, Drake’s compilation\\nand dispatching algorithms are built around these techniques. While the focus of this paper\\nis on dispatchable execution, the techniques surrounding labeled distance graphs hold the\\npromise of extending a wide range of reasoning methods involving graph algorithms to\\ninclude choice.\\nIn practical terms, Drake’s compact encoding provides a reduction in the size of the plan\\nused by the dispatcher by over two orders of magnitude for problems with around 2,000\\ncomponent STNs, as compared to Tsamardinos et al.’s work (2001). This size reduction\\ncomes at a modest increase in the run-time latency, making Drake a useful addition to the\\navailable executives.\\n609\\nConrad & Williams\\n1.1 Overview of the Problem\\nDrake takes as its input a Labeled STN, which is a temporal constraint representation\\nwith choices; in Section 3 we discuss how Labeled STNs can encode choices between sub-\\nplans, temporal constraints, and resource assignment, and mappings to related frameworks.\\nDrake’s output is a dynamic execution of the plan, where it determines in real-time when to\\nexecute the events, such that at the end of the plan the execution times are consistent with\\nevery temporal constraint implied by at least one complete set of choices, barring unforeseen\\ndisturbances. If outside disturbances make every possible execution inconsistent, then Drake\\nsignals the failure as soon as all possible solutions are rendered inconsistent.\\nSection 3 provides a formal deﬁnition of Labeled STNs; essentially, it is a collection of\\nevents to schedule and the constraints the executive must follow. The events may be con-\\nstrained with simple temporal constraints, which limit the diﬀerence in the scheduled times\\nof two events. Furthermore, the Labeled STN speciﬁes discrete choices, where assignments\\nto the choices may imply additional simple temporal constraints.\\nThroughout this paper, we use the following simple example, which includes a choice\\nbetween sub-plans.\\nExample 1.1 A rover has 100 minutes to work before a scheduled contact with its oper-\\nators. Before contact, the rover must drive to the next landmark, taking between 30 and\\n70 minutes. To ﬁll any remaining time, the rover has two options: collect some samples\\nor charge its batteries. Collecting samples consistently takes 50 to 60 minutes, whereas\\ncharging the batteries can be usefully done for any duration up to 50 minutes.\\n□\\nC\\nA\\nB\\nE\\nF\\nD\\nAlways:\\nDrive, [30, 70]\\nAlways: [0, 100]\\nAlways: [0, 0]\\nIf collecting: Collect\\nSamples, [50, 60]\\nIf collecting: [0, 0]\\nIf charging:\\nCharge [0, 50]\\nIf charging: [0, 0]\\nFigure 1.1: This informal Labeled STN depicts Example 1.1. The rover needs to drive, then\\neither collect samples or charge its batteries within a certain time limit.\\n610\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nFigure 1.1 shows an informal representation of the Labeled-STN corresponding to this\\nplan.\\nThe notation [l, u] on an edge from vertex X to Y means that the diﬀerence in\\nthe execution times of the events lies in the given interval, which expresses the constraint\\nl ≤ Y − X ≤ u. In this ﬁgure the text explains which temporal constraints are implied by\\nwhich choice; we develop precise notation later. There are two types of constraints drawn,\\nthose that are always required, and those only required if the rover is either collecting\\nsamples or charging.\\nConsider the following correct output execution sequence for the rover problem. In this\\nexample, we focus on the form of the executive’s output, deferring the presentation of the\\ndecision-making strategy until later.\\nExample 1.2 Drake starts executing the plan, arbitrarily denoting the starting time as\\nt = 0. At that time, it instructs the system to begin the drive activity, indicating that\\nthe drive should take 40 minutes. The executive then waits until the system responds that\\nthat the drive has completed, at time t = 45. Then Drake selects the sample collection\\noption, which had not been determined before, and initiates the activity with a duration of\\n50 minutes. At t = 95, the sample collection completes, ﬁnishing the plan within the time\\nlimit of 100 minutes.\\n□\\n1.2 Approach: Exploiting Shared Structure through Labeling\\nDrake’s strategy during compilation is to begin with the Labeled STN, a concise statement\\nof the temporal constraints and the choices in the plan. Then, Drake constructs the La-\\nbeled Distance Graph associated with the Labeled STN, yielding a single graph structure\\nrepresenting all the possible choices and constraints. Next, Drake’s compiler computes the\\ndispatchable form of the problem, which is also a Labeled Distance Graph. This compila-\\ntion is performed in a uniﬁed process that is able to exploit any similarities between the\\nchoices to make the representation compact. In contrast, the prior work of Tsamardinos et\\nal. (2001), breaks the input plan into independent STNs, hence their compilation strategy\\ncannot exploit any similarities or shared structures between the choices. There are patho-\\nlogical cases, where every choice is completely unrelated, where there are no similarities\\nfor Drake to exploit. However, we expect that nearly every real-world or human designed\\nplan has some degree of shared structure, because a plan usually has some unifying idea\\nwhich the choices are designed to accomplish. Indeed, we expect that most real plans have\\nsigniﬁcant similarities, allowing Drake to perform well. This section gives an overview of\\nthe intuition behind the representation Drake uses and the similarities that Drake exploit.\\nBefore we continue discussing the rover example, consider Figure 1.2, which depicts\\na small STN, its associated distance graph, and the dispatchable distance graph that is\\nthe result of compilation. The set of events in the STN are represented as vertices in the\\ndistance graph. Upper bounds in the STN induce edges in the forward direction, weighted\\nwith the upper bound, and lower bounds induce edges in the reverse direction weighted\\nwith the negative of the lower bound. The distance graph in Figure 1.2b is compiled, and\\nthe compiler outputs a new distance graph that contains representations of the constraints\\nneeded by the dispatcher. The dispatchable form in Figure 1.2c is used by a dispatcher;\\nexecution times are propagated through the edges to determine when other events may be\\nexecuted.\\n611\\nConrad & Williams\\nA\\nC\\nB\\n[3, 3]\\n[−2, 5]\\n[5, 10]\\n(a) Input STN\\nA\\nC\\nB\\n3 -3\\n5\\n2\\n10\\n-5\\n(b) Associated distance graph\\nA\\nC\\nB\\n3 -3\\n8\\n-5\\n(c) Dispatchable graph\\nFigure 1.2: A simple example of reformulating an STN into its associated distance graph\\nand its dispatchable form.\\nThe rover example has a single binary choice, hence for this problem Tsamardinos et al.’s\\n(2001) algorithm separates the two possible STNs then compute their associated distance\\ngraphs, which are shown in Figure 1.3. Note the repetition of certain edges in both graphs,\\nfor example, the edge A → F, which is present throughout their compilation and dispatch\\nprocess. Plans with more choices can have an exponential number of repetitions, which can\\nbe costly, and which Drake is designed to eliminate.\\nAn informal version of the Labeled Distance Graph associated with the rover example is\\nshown in Figure 1.4. The diﬀering constraints that result from the two possible assignments\\nto the choice are distinguished by annotations called labels. For example, edge (B, C) has\\nweight S : 60, which indicates whenever sampling is chosen, the edge has a weight of at\\nmost 60; the value 60 is labeled with the discrete choice, S, that implies it. We gather\\nall the possible values under all the choices into Labeled Value Sets, which are placed on\\nedges. In this example, each edge has a Labeled Value Set with exactly one labeled value,\\nalthough this is not true in general. A Labeled Distance Graph is essentially a distance\\ngraph where the numeric weights are replaced with Labeled Value Sets. Although we develop\\nmore precise notation later on in this article, this version shows the intuition behind the\\napproach.\\nDrake capitalizes on this improvement by using the compact representation\\nthroughout compilation and dispatch, and this work develops the necessary machinery.\\nThis paper is organized as follows. Section 2 discusses related work on temporal execu-\\ntives and provides background on truth maintenance. Section 3 deﬁnes Labeled STNs and\\ntheir correct dynamic execution, specifying the problem Drake solves. Section 4 recalls the\\n612\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nC\\nA\\nB\\nE\\nF\\nD\\n70\\n-30\\n100\\n0\\n0\\n0\\n60\\n-50\\n0\\n0\\n(a) Distance graph if collecting samples\\nC\\nA\\nB\\nE\\nF\\nD\\n70\\n-30\\n100\\n0\\n0\\n0\\n50\\n0\\n0\\n0\\n(b) Distance graph if charging\\nFigure 1.3: The Tsamardinos et al.\\n(2001) style distance graphs associated with the\\nLabeled-STN in Figure 1.1.\\nlink between STNs and distance graphs, and provides the labeled version of distance graphs,\\nwhich Drake uses for reasoning. Section 5 presents the Labeled Value Set Maintenance Sys-\\ntem, completing the foundation of labeled techniques. Section 6 details the dispatcher and\\nSection 7 develops Drake’s compilation algorithm. Finally, Section 8 provides some theoret-\\nical and experimental performance results, and Section 9 gives some concluding remarks.\\n2. Related Work\\nBefore developing Drake, we give an overview of some relevant literature in the two major\\nareas Drake draws from: scheduling frameworks and truth maintenance.\\n613\\nConrad & Williams\\nC\\nA\\nB\\nE\\nF\\nD\\nA: 70\\nA:−30\\nA: 100\\nA: 0\\nA: 0\\nA: 0\\nS: 60\\nS: −50\\nS: 0\\nS: 0\\nC: 50\\nC: 0\\nC: 0\\nC: 0\\nFigure 1.4: An informal labeled distance graph for the rover example.\\nA:, S: and C:,\\ncorrespond to weights that hold always, with sampling, and with charging,\\nrespectively.\\n2.1 Scheduling Frameworks and Executives\\nAs stated in the introduction, to achieve robustness, we need executives that make decisions\\ndynamically and with low latency over expressive temporal representations.\\nThere are\\nknown methods for manipulating and reasoning over Simple Temporal Networks eﬃciently,\\nwhich have been used as the foundation for most work in temporal executives. Furthermore,\\nnumerous eﬀorts have formulated and developed extensions of STNs to include other useful\\nproperties, including uncertainty, preferences, and discrete choices. We brieﬂy review some\\nof these eﬀorts. Since our work focuses on discrete choices, we discuss several eﬀorts to build\\ndynamic executives for these plans in more detail. These executives typically use one of\\ntwo approaches: they reason on all plans in parallel, or switch between plans incrementally.\\nHowever, these approaches, while promising, are typically either too memory intensive or\\nmay have high latency.\\nTemporal Constraint Networks (TCNs), formalized by Dechter, Meiri and Pearl (1991),\\ncapture many of the qualitative and metric temporal representations introduced by the AI\\ncommunity. A restricted type of TCN, the Simple Temporal Network, is used throughout\\nrecent work in temporal planning, temporal reasoning, and scheduling. Muscettola, Morris,\\nand Tsamardinos (1998) proposed framework for low-latency dynamic execution: a pre-\\nprocessing step called compilation and a run-time component called dispatch. Tsamardinos,\\nMuscettola, and Morris (1998) later provided a faster compilation algorithm. Further work\\nhas also developed more eﬃcient methods for testing consistency of STNs (Xu & Choueiry,\\n2003; Planken, de Weerdt, & van der Krogt, 2008).\\n614\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nDechter et al. (1991) also proposed Temporal Constraint Satisfaction Problems, which\\ninclude discrete choices that alter the simple interval constraint between particular pairs of\\nevents; each pair may have a choice of interval constraints, but choices for each pair of events\\nmust be independent. Stergiou and Koubarakis (2000) loosened this structural restriction,\\ndeveloping the Disjunctive Temporal Network (DTN). Tsamardinos, Pollack, and Ganchev\\n(2001) presented the ﬁrst dynamic executive for DTNs, which functions by generating the\\ncomponent STNs implied by all combinations of the discrete choices and compiling them\\nindependently, creating an exponential growth in memory use with respect to the number\\nof choices.\\nAnother important line of extension to STNs is the Simple Temporal Network with Un-\\ncertainty (STNU). Morris, Muscettola, and Vidal (2001) proved that an executive can test\\nfor consistency of an STNU and compile it into dispatchable form in polynomial time. Mor-\\nris (2006) described a more eﬃcient algorithm for testing dynamic controllability. Hunsberg\\n(2009, 2010) corrected a ﬂaw in the previous deﬁnitions and described an execution strategy\\nusing the more eﬃcient dynamic controllability algorithm. Venable and Yorke-Smith (2005)\\nadded temporal uncertainty to DTNs. Tsamardinos (2002) introduced a probabilistic for-\\nmulation of uncertainty in STNs. Conrad (2010) presents an extension of Drake to DTNs\\nwith uncertainty.\\nTsamardinos, Vidal, and Pollack (2003) introduced Conditional Temporal Problems\\n(CTP), adding uncontrollable discrete choices.\\nThe executive cannot control, but may\\nonly observe the values of some discrete choices at designated parts of the plan. Some of\\ntheir notation is quite similar to that used here for Drake, but there are two important\\ndiﬀerences. First, a CTP is a strictly harder problem, since Drake is not concerned with\\nuncontrollable choices, meaning that their algorithm does more work than is necessary for\\nthe simpler case.\\nSecond, their algorithm does not use a compact representation; their\\nalgorithm for consistency checking requires enumerating the possible scenarios. An open\\nproblem for future research is to adapt their more general algorithms to take advantage of\\nthe compactness of the Labeled Distance Graph.\\nAnother useful feature added to STNs is preferences. Khatib, Morris, Morris, and Rossi\\n(2001) introduced a formulation including preferences for event execution times within the\\nsimple interval bounds allowed by an STN, adding a notion of quality to the existing notion\\nof consistency. Rossi, Venable, and Yorke-Smith (2006) discuss simultaneous handling of\\nuncertainty and preferences.\\nKim, Williams, and Abramson (2001) present Temporal Plan Networks, a representation\\nthat provides simple temporal constraints over durations combined in series, parallel, and\\nwith choice, where each choice has speciﬁed costs. Eﬃnger (2006) expands this to a simple\\npreference model, in which choices and activities have associated, ﬁxed costs. Kirk is a\\ndynamic executive for TPNs. Kirk performs optimal method selection just before run-time,\\nassigning the discrete choices and then dispatches the resulting component STN. If some\\ndisturbance invalidates the STN that Kirk chose, then Kirk selects a new STN consistent\\nwith the execution thus far. Further research developed incremental techniques to allow\\nKirk to re-plan with lower latency (Shu, Eﬃnger, & Williams, 2005; Block, Wehowsky, &\\nWilliams, 2006).\\nShah and Williams (2008) present Chaski, an executive that dynamically dispatches\\nplans with task assignment over heterogeneous, cooperative agents, represented by a TCN,\\n615\\nConrad & Williams\\nby removing some redundant data structures and computations performed by Tsamardinos\\net al.’s (2001) algorithm. Shah and Williams point out that the component STNs of real-\\nworld TCNs often diﬀer by only a few constraints, allowing a compact representation. They\\nrecord all the component STNs by storing a single relaxed STN and maintaining a list of\\nmodiﬁcations to the relaxed STN that recover each of the original component STNs. By\\navoiding redundant records of shared constraints, their results show dramatic improvements\\nin performance.\\nOur work is inspired by the observation that this technique, although\\ndistinct, bears some resemblance to the environment labeling scheme that is employed by\\nan Assumption Based Truth Maintenance System (ATMS). We speciﬁcally adapted ATMS\\nideas to work with a more general problem formulation than Chaski, expecting to see similar\\nperformance improvements.\\nCombi and Posenato (2009, 2010) discuss applications of dynamic executives to business\\nwork ﬂows, which include ﬂexibility over time of execution, hierarchical choice over execution\\npaths, and temporal uncertainty. Their formalism for plans, Workﬂow Schemata, is closely\\nrelated to DTN, STNU, and TPN frameworks, and they discuss variants of compilation\\nand dispatching algorithms specialized to their representations. Their work describes an\\nintriguing notion they call history-dependent controllability.\\nUnder this model, if event\\nX starts one of two sub-plans, the executive may not invalidate either sub-plan until it\\nexecutes X and begins one of them. Drake does not impose a similar requirement, but the\\nrequirement is certainly useful for preserving the executive’s ﬂexibility over future choices\\nas the execution unfolds. Their algorithms for testing controllability enumerate the possible\\nchoices, and therefore suﬀers from memory growth.\\nSmith, Gallagher, and Zimmerman (2007) describe a distributed dynamic executive for\\nreal world plans in the C TAEMS language. That representation uses STN temporal se-\\nmantics and includes other features intended to represent cooperative multi-agent plans.\\nTheir language features a rich and practical notion of activity failure not present in STNs,\\nincluding the potential for interruption of activities. The executive is given discrete choices\\nover method selection and resource allocation, and attempts to maximize the utility of the\\noverall plan. Their preference model accounts for partial or total method failures and sup-\\nports diﬀerent functions for accumulating reward, for example, summing or maxing. Their\\nexecutive uses a re-planning strategy, similar to Kirk, which is enhanced by Hiatt, Zimmer-\\nman, Smith, and Simmons (2009) with a type of compile-time analysis called strengthening.\\nThis analysis performs a type of local repair that attempts to make the plan more robust\\nto uncertainties or activity failures.\\nThere are two central approaches to dynamic executives that include discrete choices.\\nFirst, Tsamardinos et al.’s (2003) CTPs, Tsamardinos et al.’s (2001) DTN dispatcher, and\\nCombi et al. (2010) use a compile-time analysis to compute the implied constraints of every\\npossible plan and explicitly reason over them at run-time. Second, Kim et al. (2001), and\\nSmith et al. (2007) focus on a single, potentially optimal, assignment to the choices, and if\\nthat becomes infeasible, they incrementally re-plan and extract a new plan. Both methods\\nhave shortcomings, since explicit compilation is memory intensive and re-planning steps can\\nbe computationally intensive, especially if the executive is forced to re-plan often. Drake,\\nlike Chaski, provides a middle ground by working with a compilation strategy that has a\\nreduced memory footprint.\\n616\\nDrake: An Efficient Executive for Temporal Plans with Choice\\n2.2 Background on ATMSs\\nStallman and Sussman (1977) introduced the profoundly useful idea of tracing the depen-\\ndency of deductions in a computerized aid to circuit diagnosis, in order to focus the search\\nfor a consistent component mode assignment during transistor circuit analysis. The depen-\\ndencies it computes from solving the equations allow it to rapidly ﬁnd those choices that\\nmight be responsible for the detected failure. They generalize this approach to combina-\\ntorial search, introducing the dependency-directed backtracking algorithm, which ensures\\nthat when a conﬂict is found that the search backs up far enough to ensure that the newly\\nfound inconsistency is actually removed.\\nDoyle (1979) introduced Truth Maintenance Systems (TMSs) as a domain independent\\nmethod for supporting dependency-directed backtracking. The TMS represents data, their\\njustiﬁcations, and provides the ability to revise beliefs when assumptions change or con-\\ntradictions arise. For example, consider a problem solver designed to search for a solution\\nto a constraint satisfaction problem. When determining whether a particular solution is\\nconsistent, the problem solver will perform a chain of inferences, providing the TMS with\\nthe justiﬁcation for each step. If an inconsistency is found, the problem solver selects a new\\ncandidate solution, and the TMS uses the justiﬁcations to determine which of the inferences\\nstill hold under the new candidate and which must be recomputed to account for new cir-\\ncumstances. The TMS continually determines whether a particular datum, a general term\\nfor any fact that arises in problem solving, is in or out, that is, currently believed true, or\\nnot currently believed.\\nLater work relaxes the goal of maintaining a single, consistent assignment to all data\\nof in or out, and instead tracks the contexts in which particular facts hold, even if those\\ncontexts may be mutually exclusive. McDermott (1983) uses beads to state a context, which\\nis a particular set of choices or assumptions on which the reasoning might rely, and provides\\ndata pools that specify all the facts that hold in that context. De Kleer (1986) develops the\\nAssumption-based Truth Maintenance System, which uses a similar idea, but changes the\\nterminology to use environments and labels to specify contexts. The ATMS maintains a set\\nof minimal inconsistent environments, called conﬂicts or no-goods. These conﬂicts help the\\nsystem to avoid performing inferences for contexts that are already known to be inconsistent,\\nand the minimality of the conﬂict set makes the procedure tractable. The ATMS is designed\\nto simultaneously ﬁnd the logical consequences of all possible combinations of assumptions,\\nin contrast to the TMS, which focuses on ﬁnding any one set of assumptions that solve the\\nproblem of interest. Hence, the ATMS is well suited as the foundation for an executive\\nthat is intended to consider all possible choices simultaneously without incurring latency\\nfor switching between choices. Finally, there is some development of the idea that when\\nworking with inequalities, the ATMS only needs to keep tightest bounds on the inequalities,\\nwhich we use extensively; this concept was described by Goldstone (1991) as hibernation.\\nWe leave our review of the details of the ATMS to later sections, as we develop Drake’s\\nmachinery in depth.\\n3. Dynamic Execution of STNs and Labeled STNs\\nNow we are prepared to present the formal representation of the temporal plans that Drake\\nuses. Plans are composed of actions that need to be performed at feasible times, where we\\n617\\nConrad & Williams\\ndeﬁne feasible times through constraints on the start and end times of the plan activities.\\nOur work builds upon Simple Temporal Networks, and we begin by explaining how STNs\\nconstrain the events of a plan and their feasible execution. We extend these deﬁnitions to\\ninclude discrete choices, thereby constructing Labeled STNs.\\n3.1 Simple Temporal Networks\\nSimple Temporal Networks provide a framework for eﬃciently reasoning about a limited\\nform of temporal constraints. A simple temporal network is deﬁned as a set of events related\\nby binary interval constraints, called simple interval constraints (Dechter et al., 1991).\\nDeﬁnition 3.1 (Event) An event is a real-valued variable, whose value is the execution\\ntime of the event.\\n□\\nDeﬁnition 3.2 (Simple Interval Constraint) A simple interval constraint ⟨A, B, l, u⟩\\nbetween two events A and B requires that l ≤ B − A ≤ u, denoted, [l, u].\\n□\\nBy convention, u is non-negative. The lower bound, l may be positive if there is a strict\\nordering of the events, or negative if there is not a strict ordering. Positive or negative\\ninﬁnities may be used in the bounds to represent an unconstrained relationship.\\nDeﬁnition 3.3 (Simple Temporal Network) A Simple Temporal Network ⟨V, C⟩ is\\ncomprised of a set of events V and a set of simple interval constraints C.\\nA schedule\\nfor an STN is an assignment of a real number to each event in V , representing the time\\nto schedule each event. The Simple Temporal Problem (STP) is, given an STN ⟨V, C⟩,\\nreturn a consistent schedule if possible, else return false. A consistent schedule is a schedule\\nthat satisﬁes every constraint in C. If and only if at least one solution exists, the STN is\\nconsistent.\\n□\\nDeﬁnition 3.4 (Dynamic Execution) A dynamic execution of an STN is the construc-\\ntion of a consistent schedule for the STN in real-time. The executive decides at time t − ϵ\\nwhether to execute any events at time t, for some suitably small ϵ. If at some time, there\\nare no longer any remaining consistent schedules, return false immediately. The executive\\nmay arbitrarily select any consistent schedule.\\n□\\n3.2 Adding Choice: Labeled STNs\\nThis section deﬁnes our key representational concept, Labeled STNs, a variant of STNs that\\nis designed to include discrete choices between temporal constraints. Although equivalent\\nin expressiveness to Disjunctive Temporal Networks, Labeled STNs provide more consistent\\nterminology for Drake, and their corresponding labeled distance graphs make it easier to\\nextend standard STN and weighted graph algorithms to include choice. We show the precise\\nconnection between DTNs and Labeled STNs at the end of this section. These deﬁnitions\\nare used throughout the compilation and dispatching algorithms presented later in this\\nwork.\\nThe input problem needs a succinct way to state what the choices of the input plan are\\nand what the possible options are that the executive may select between. We accomplish\\nthis through a set of ﬁnite domain variables.\\n618\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nDeﬁnition 3.5 (Choice Variables) Each choice is associated with a ﬁnite domain vari-\\nable xi. Each of these variables has a domain with size equal to the number of options\\nof that choice. X is the set of all the variables for a particular problem. An assignment\\nis a selection of a single option for a choice, represented as an assignment to the choice’s\\nassociated choice variable.\\n□\\nExample 3.6 In the rover example, there is a single choice with two options, leading to a\\nsingle variable x ∈ {collect, drive}. We might assign x = collect to represent the choice of\\ncollecting samples.\\n□\\nIn general, Drake will reason about the implications of combinations of assignments.\\nTo specify these assignments to the choice variables, Drake uses environments.\\nFollow-\\ning the ATMS, Drake annotates, or “labels” interval constraints and edge weights with\\nenvironments specifying when they are entailed.\\nDeﬁnition 3.7 (Environment) An environment is a partial assignment of the choice\\nvariables in X, written e = {xi = dij, ...}. An environment may have at most one assignment\\nto each variable to be consistent. A complete environment contains an assignment to every\\nchoice variable in X. An empty environment provides no assignments and is written {}.\\nWe denote the set of possible environments as E and the set of complete environments as\\nEc. The length of an environment is the number of assigned variables, denoted |e|.\\n□\\nExample 3.8 In a problem with two choice variables x, y ∈ {1, 2}, some possible environ-\\nments are {}, {x = 1}, {y = 2}, and {x = 1, y = 2}.\\n□\\nIn a Labeled STN, diﬀerent assignments to the choice variables entail diﬀerent temporal\\nconstraints, which we represent with labeled simple interval constraints.\\nDeﬁnition 3.9 (Labeled Simple Interval Constraint) A labeled simple interval con-\\nstraint is a tuple ⟨A, B, l, u, e⟩ for a pair of events A and B, real valued weights l and u\\nand environment e ∈ E. This constraint states that, if the assignments in e hold, then the\\nsimple interval constraint ⟨A, B, l, u⟩ is entailed.\\n□\\nThen a Labeled STN is deﬁned analogously to an STN, but extended to include choices\\nand labeled constraints.\\nDeﬁnition 3.10 (Labeled Simple Temporal Network) A Labeled STN ⟨V, X, C⟩ is a\\nset of events V , a set of choice variables X, and a set of labeled simple interval constraints\\nC. As with STNs, a schedule for a Labeled STN is an assignment of real numbers to each\\nevent, indicating the time to execute the event. This schedule is consistent if there is a\\nfull assignment to the choice variables so that the schedule satisﬁes every simple interval\\nconstraint entailed by a labeled simple interval constraint.\\n□\\nExample 3.11 (Rover Problem as a Labeled STN) The rover problem of Example\\n1.1 has a single choice, with two options, collecting samples or charging the batteries.\\nUse a single choice variable x ∈ {collect, charge} to represent the choice. The network\\nhas events {A, B, C, D, E, F}. The labeled simple interval constraints below are written\\ne : l ≤ B − A ≤ u.\\n619\\nConrad & Williams\\n{} : 0 ≤ F − A ≤ 100\\n(1)\\n{} : 30 ≤ B − A ≤ 70\\n(2)\\n{} : 0 ≤ F − E ≤ 0\\n(3)\\n{x = collect} : 50 ≤ C − B ≤ 60\\n(4)\\n{x = collect} : 0 ≤ E − C ≤ 0\\n(5)\\n{x = charge} : 0 ≤ D − B ≤ 50\\n(6)\\n{x = charge} : 0 ≤ E − D ≤ 0\\n(7)\\nThis notation states that the inequalities on lines 1-3 must always hold, lines 4-5 must\\nhold if the executive decides to collect samples, and lines 6-7 must hold if the executive\\ndecides to charge the batteries. The schedule given in Example 1.2, A = 0, B = 45, C =\\n95, D = 45, E = 95, F = 95, is consistent with the full assignment x = collect.\\nThe\\nconstraints on lines 1-3 hold for any choice, so necessarily must hold here. Lines 4-5 give\\nconstraints with environments whose assignments are all given in the full assignment, so\\ntheir simple interval constraints must hold, and do.\\nLines 6-7 give constraints with an\\nenvironment that include diﬀerent assignments than our full assignment, so the constraints\\ndo not need to hold for the schedule to be consistent.\\n□\\nDrake provides a dynamic execution of a Labeled STN, making decisions at run-time,\\nas late as possible.\\nDeﬁnition 3.12 (Dynamic execution of a Labeled STN) A dynamic execution of a\\nLabeled STN is the simultaneous real-time construction of a full assignment and a corre-\\nsponding consistent schedule. The executive decides at time t−ϵ whether it will execute any\\nevents at time t, for some suitably small ϵ. Possible assignments to choices are only elim-\\ninated from consideration as necessary to schedule events. The executive may arbitrarily\\nselect any consistent schedule.\\n□\\nExample 1.2 gave a dynamic execution of the rover problem because the dispatcher se-\\nlected choices and scheduling times dynamically, only choosing the option to collect samples\\nimmediately before the start of the sample collection activity.\\nWe conclude this section by discussing the equivalence of DTNs and Labeled STNs,\\nto allow for easier comparison to prior work. Recall the deﬁnition of DTNs (Stergiou &\\nKoubarakis, 2000).\\nDeﬁnition 3.13 (Disjunctive Temporal Network) A Disjunctive Temporal Network\\n⟨V, C⟩ is a set of events V and a set of disjunctive constraints C. Each disjunctive constraint\\nCi ∈ C is of the form\\nci1 ∨ ci2 ∨ ... ∨ cin,\\n(8)\\nfor positive integer n and each cij is a simple interval constraint. As before, a schedule is\\nan assignment of a time to each event. The schedule is consistent if and only if at least\\none simple interval constraint cij is satisﬁed for every disjunctive constraint Ci. A DTN is\\nconsistent if and only if at least one consistent schedule exists.\\n□\\n620\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nThe DTN and Labeled STN deﬁnitions are analogous, except for the diﬀerence in how\\nchoices are speciﬁed. We can construct a Labeled STN equivalent to any DTN by creating\\na choice variable for each disjunctive constraint, with one value for each disjunct. Thus,\\nxi = 1...n. Each disjunctive constraint in the DTN, cij is labeled with environment xi = j.\\nNon-disjunctive constraints are labeled with {}.\\nExample 3.14 Consider a DTN with three events, A, B, C. Assume there are two dis-\\njunctive constraints, ⟨A, B, 3, 5⟩, and ⟨B, C, 0, 6⟩ ∨ ⟨A, C, −4, −4⟩. Then the correspond-\\ning Labeled STN would have one binary choice, represented by x ∈ {1, 2}.\\nIt would\\nhave three labeled simple interval constraints: ⟨A, B, 3, 5, {}⟩, ⟨B, C, 0, 6, {x = 1}⟩, and\\n⟨A, C, −4, −4, {x = 2}⟩.\\n□\\nTo see the reverse construction, note that the DTN speciﬁes a set of simple interval\\nconstraint in conjunctive normal form. Labeled STNs allow the speciﬁcation of somewhat\\nmore complex boolean expressions, but all boolean expressions are reducible to conjunctive\\nnormal form, so Labeled STNs are not more expressive.\\nThe mapping from DTNs to\\nLabeled STNs is straightforward because we construct a Labeled STN that directly uses a\\nconjunctive normal form expression.\\nExample 3.15 Consider the rover problem, given as a Labeled STN in Example 3.11. If\\nwe represent the constraint 0 ≤ F −A ≤ 100 as CF,A, then we can identify that this problem\\nhas the following boolean form\\nCF,A ∧ CB,A ∧ CF,E ∧ ((CC,B ∧ CE,C) ∨ (CD,B ∧ CE,D))\\n(9)\\nWe could expand this to conjunctive normal form\\nCF,A ∧ CB,A ∧ CF,E ∧ (CC,B ∨ CD,B) ∧ (CC,B ∨ CE,D) ∧ (CD,B ∨ CE,C)\\n(10)\\nFrom this form, we can construct a DTN directly.\\n□\\nThus, like the DTN, Labeled STNs provide a rich notion of choice. Given the deﬁnition\\nof the problem that Drake solves in this section and the introduction of environments, the\\nnext two sections develop the labeled machinery Drake uses to eﬃciently perform temporal\\nreasoning.\\n4. Distance Graphs and Temporal Reasoning\\nDechter et al. (1991) showed that STN reasoning can be reformulated as a shortest path\\nproblem on an associated weighted distance graph. This connection is important because\\nweighted graphs are easy to manipulate and have well developed theory and eﬃcient al-\\ngorithms, hence most practical algorithms for STNs are based on this connection. Drake\\nfollows the prior literature, in that it frames the temporal reasoning as a labeled version\\nof shortest path problems. This section begins to develop the formalism for Labeled Value\\nSets and Labeled Distance Graphs, which allow us to compactly represent the shortest path\\nproblems and algorithms. We begin by reviewing the transformation for STNs.\\n621\\nConrad & Williams\\nDeﬁnition 4.1 (Distance Graph associated with an STN) A distance graph associ-\\nated with an STN is a pair ⟨V, W⟩ of vertices V and edge weights W.\\nEach event is\\nassociated with a vertex in V . The vertices exactly correspond to the events of the STN.\\nThe weights are a function V × V → R. The simple temporal constraint l ≤ B − A ≤ u is\\nrepresented by the edge weights W(B, A) = u and W(A, B) = −l.\\n□\\nFigure 1.2 shows an example of the conversion from an STN to a distance graph. Re-\\ncall that an STN is consistent if and only if its associated distance graph does not have\\nany negative cycles (Dechter et al., 1991). The compilation algorithm for an STN takes\\nthe associated distance graph as an input and outputs another distance graph that is the\\ndispatchable form.\\nDeﬁnition 4.2 (Dispatchable form of a distance graph) A weighted distance graph\\nis the dispatchable form of an STN if an executive may dynamically execute the STN in\\na greedy fashion to construct a consistent schedule using only local propagations.\\nThe\\ndispatchable form is minimal if all the edges are actually needed by the dispatcher for\\ncorrect execution.\\n□\\nThe all-pairs shortest path (APSP) graph of the distance graph associated with an STN\\nis a dispatchable form because it explicitly contains all possible constraints of the STN\\n(Muscettola et al., 1998). The minimal form is computable by performing pruning on the\\nAPSP graph.\\nWe begin building the labeling formalism by deﬁning labeled value pairs. Just as we\\nlabeled simple temporal constraints with environments, labeled values associate values with\\nenvironments.\\nDeﬁnition 4.3 (Labeled Value Pair) A Labeled Value Pair is a pair (a, e), where a is a\\nvalue and e ∈ E is an environment. The value a is entailed (usually as an assignment or an\\ninequality bound) under all environments where the assignments in e hold.\\n□\\nDuring compilation and dispatch, Drake uses labeled value pairs to track real valued\\nbounds, so a ∈ R.\\nDuring compilation, when performing shortest path computations,\\nDrake tracks predecessor vertices, so the value may be a pair (b, v) ∈ R × V .\\nHaving\\nthese two types of values complicates our discussion somewhat, but is necessary for the\\ncompilation algorithms, and allows an elegant implementation of the relaxation algorithm\\nfor directed, weighted graphs. The ATMS strategy of associating environments with values\\nis well founded for any arbitrary type of value, so both choices are sound. Labeled value\\npairs always use minimal environments, that is, the environment speciﬁes the smallest set\\nof assignments possible for the implication to be true. This minimality is critical for the\\nlabeling system to be eﬃcient.\\nExample 4.4 If there is a choice variable x ∈ {1, 2}, then (3, {x = 1}) is a real valued\\nlabeled value pair. Similarly, if A is an event, ((2, A), {x = 2}) is a possible predecessor\\ngraph labeled value pair.\\n□\\nNow we have arrived at a crucial contribution of this paper: extending domination into\\nthe labeled space. If we have the inequality B−A ≤ 4 ≤ 5, then it is clear that we only need\\n622\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nto keep the dominant value, the four, and may discard the ﬁve. Shortest path algorithms\\nwidely use the concept of dominance to propose many possible paths and keep only the\\ntightest one. When applied to labeled value pairs, dominance involves an ordering on two\\nparts, the value and the environment. In Drake, values are always ordered using real valued\\ninequalities, either ≤ or ≥. This paper mostly uses ≤, except when reasoning on lower\\nbounds during dispatch, where explicitly noted. When necessary, a direct replacement of\\nthe inequality direction suﬃces to extend the deﬁnitions. Environments are ordered through\\nthe concept of subsumption.\\nDeﬁnition 4.5 (Subsumption of Environments) An environment e subsumes e′ if for\\nevery assignment xi = dij ∈ e, the same assignment exists in e′, denoted xi = dij ∈ e′.\\n□\\nExample 4.6 An environment {x = 1, y = 2, z = 1} is subsumed by {x = 1, z = 1}\\nbecause the assignments in the later environment are all included in the former.\\n□\\nThen domination of labeled value pairs applies to both orderings simultaneously.\\nDeﬁnition 4.7 (Dominated Labeled Value Pair) Let (a, ea), (b, eb) be two labeled\\nvalue pairs where the values are ordered with ≤. Then (a, ea) dominates (b, eb) if a ≤ b and\\nea subsumes eb.\\n□\\nExample 4.8 If B − A ≤ 4 under {} and also B − A ≤ 4 under {x = 1}, then the\\nﬁrst inequality is non-dominated because its environment is less restrictive. Thus (4, {})\\ndominates (4, {x = 1}). Likewise, if B − A ≤ 2 under {x = 1}, this constraint dominates\\nthe constraint B − A ≤ 4 under {x = 1}, because the ﬁrst inequality is tighter and holds\\nwithin every environment where the second inequality holds, hence (2, {x = 1}) dominates\\n(4, {x = 1}).\\n□\\nTo represent the various values a bound may have, depending on the choices the ex-\\necutive makes, Drake collects the non-dominated labeled value pairs into a Labeled Value\\nSet.\\nDeﬁnition 4.9 (Labeled Value Set) A Labeled Value Set, L is a set of non-dominated la-\\nbeled value pairs, that is, a set of pairs (ai, ei). Thus, we may write L = {(a1, e1), ...(an, en)}.\\nNo labeled value pair in the set is dominant over another pair in the set.\\n□\\nIn any particular labeled value set, the values are all of the same type, either real values\\nor value/vertex pairs.\\nExample 4.10 Assume there is a single choice variable, x ∈ {1, 2}.\\nSome real valued\\nvariable, A ∈ R has value 3 if x = 1 and 5 if x = 2. Then A is represented by the labeled\\nvalue set, A = {(3, {x = 1}), (5, {x = 2})}.\\n□\\nFinally, we can modify distance graphs to use Labeled Value Sets instead of real values\\nfor the weights, leading to the deﬁnition of labeled distance graphs.\\n623\\nConrad & Williams\\nDeﬁnition 4.11 (Labeled Distance Graph) A labeled distance graph G is a tuple\\n⟨V, E, W, X⟩.\\nV is a set of vertices and E is a set of directed edges between the ver-\\ntices, represented as ordered pairs (i, j) ∈ V × V . W is a weight function from edges to real\\nvalued labeled value sets, where each edge (i, j) ∈ E is associated with a labeled value set\\nW(i, j). X is the description of the choices variables, deﬁning the set of environments that\\nmay appear in the labeled value sets contained in W.\\n□\\nExample 4.12 Consider a simple graph with three vertices, V = {A, B, C}. This graph\\ncontains edges E = {(A, B), (A, C), (B, C)}. There is one choice variable x ∈ {1, 2}. Edge\\n(A, B) has weight 1 regardless of the choice. Edge (A, C) is 3 if x = 1 and 7 if x = 2.\\nFinally, edge (B, C) has weight 4 if x = 2. This labeled distance graph is shown in Figure\\n4.1.\\n□\\nA\\nB\\nC\\n(1, {})\\n{(4, {x = 2})}\\n{(3, {x = 1}),\\n(7, {x = 2})}\\nFigure 4.1: A simple Labeled Distance Graph with one choice variable, x ∈ {1, 2}\\nThe association between Labeled STNs and labeled distance graphs closely parallels the\\nassociation between STNs and distance graphs.\\nDeﬁnition 4.13 (Labeled Distance Graph Associated with a Labeled STN)\\nThe labeled distance graph associated with a Labeled STN has a vertex associated with\\nevery event in the Labeled STN. For each labeled inequality, denoted e : Y − X ≤ w, where\\ne ∈ E, X, Y ∈ V , w ∈ R, and the edge (X, Y ) ∈ E. The labeled value set W(X, Y ) includes\\nthe labeled value pair (w, e) if the pair is not dominated by another (w′, e′) ∈ W(X, Y ). □\\nExample 4.14 Figure 1.4 is the essentially the labeled distance graph for the rover problem\\ndescribed in Example 1.1, except that we replace the informal notation with equivalent\\nlabeled value sets. For example, the edge (B, C) currently has weight S : 60, which we\\nreplace with the labeled value set {(60, {x = collect})}.\\n□\\nDrake compiles the input Labeled STN by creating its associated labeled distance graph\\nrepresentation and then constructing a new labeled distance graph through a transformation\\nof the input graph.\\nDeﬁnition 4.15 (Dispatchable Form of a Labeled STN) A labeled distance graph is\\nthe dispatchable form of a Labeled STN if it represents all the constraints necessary to\\naccurately perform dispatch with a greedy strategy, using only local propagations.\\n□\\n624\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nThis section constructed the labeling structures Drakes uses, beginning with labeled\\nvalue pairs and then building labeled value sets from minimal sets of labeled pairs. Labeled\\nvalue sets are used to construct labeled distance graphs, and we deﬁned how to construct the\\ngraph associated with a Labeled STN, on which Drake’s compilation algorithms operate.\\n5. Labeled Value Set Maintenance System\\nThe previous section deﬁnes the representation for labeled distance graphs and labeled value\\nsets; this section provides tools for manipulating them. The primary focus of labeled value\\nsets is to maintain only non-dominated labeled value pairs, which makes the labeled value\\nsets compact and eﬃcient. This section introduces three concepts. First, we describe how\\nto extract values from a labeled value set, called a query, allowing us to ﬁnd the dominant\\nvalue implied by an environment. Second, how to handle assignments to choices that are\\ninconsistent, which are called conﬂicts. Third, how to apply functions to labeled value sets,\\nwhich allows us to perform computations on them directly.\\nFirst we deﬁne the query, which lets us extract the precise dominant value(s) that are\\nguaranteed to hold under a particular environment.\\nDeﬁnition 5.1 (Labeled Value Set Query) The query operator A(e) is deﬁned for la-\\nbeled value set A and e ∈ E. The query returns a set of values including ai from the pair\\n(ai, ei) ∈ A, where ei subsumes e, if and only if there is no other pair (aj, ej) ∈ A such that\\nej subsumes e and aj < ai. If no environment ei subsumes e, then A(e) returns ∅.\\n□\\nHere we introduce a convention: when using pairs of real values and vertices for the\\nvalues of labeled value pairs, we consider inequalities over the pairs to be deﬁned entirely\\nby the real value. Thus, (5, A) < (6, E) and (5, A) ≮ (5, E).\\nExample 5.2 Assume that there are two choice variables x, y ∈ {1, 2}, and a real valued\\nvariable A represented by a labeled value set, where A = {(3, {x = 1}), (5, {})}. A({x =\\n1, y = 1}) = 3 because the input environment is subsumed by both environments in the\\nlabeled value set, but three is dominant. A({x = 2, y = 2}) = 5 because only the empty\\nenvironment in the labeled value set subsumes the input environment.\\n□\\nExample 5.3 Let A be a labeled value set with values that are pairs of real values and\\nvertices, A = {((3, A), {x = 1}), ((5, A), {y = 1}), ((5, B), {})}. Then A({x = 1, y = 2}) =\\n{(3, A)} because all the environments are subsumed, but (3, A) < (5, A) and (3, A) < (5, B).\\nA({x = 2, y = 1}) = {(5, A), (5, B)} because the tighter value is not applicable and neither\\nvalue with real part ﬁve is less than the other.\\n□\\nThe query operator deﬁnes the expansion from the compact form to an explicit listing of\\nthe values for each environment. Although Drake never performs this expansion, it is useful\\nfor determining the correct behavior of a Labeled Value Set when designing algorithms.\\nSecond, we consider conﬂicts, an important function of an ATMS that allows it to track\\ninconsistent environments. In our case, an inconsistent environment signals an inconsistent\\ncomponent STN. The standard strategy in an ATMS is to keep a list of minimal conﬂicts,\\nalso referred to as no-goods (de Kleer, 1986).\\n625\\nConrad & Williams\\nDeﬁnition 5.4 (Conﬂict) A conﬂict is an environment e such that if e subsumes e′, then\\ne′ is inconsistent. The conﬂict e is minimal if there is no e′′ ∈ E such that e′′ ⊂ e, such that\\ne′′ is a conﬂict (Williams & Ragno, 2007).\\n□\\nExample 5.5 For example, the compilation process might determine that x = 1 and y = 1\\nare contradictory choices, and cannot be selected together during any execution. Then,\\n{x = 1, y = 1} is a conﬂict.\\n□\\nOften, reasoning algorithms keep a cache of conﬂicts to avoid performing work on envi-\\nronments that were previously discovered to contain an inconsistency. In practice, the set of\\nconﬂicts can become large and unwieldy, leading some practical systems to keep only a sub-\\nset of conﬂicts, using principles such as temporal locality to maintain a small, useful cache.\\nSince the cache of conﬂicts is incomplete, the cache can miss, requiring the problem solver to\\nre-derive an inconsistency, but missing the cache would never lead to incorrectly accepting\\nan inconsistent solution. A good example of this is conﬂict learning in the widely studied\\nSAT-solver MiniSAT (En & Srensson, 2004). During real-time execution, an incomplete\\ncache of conﬂicts would require Drake to perform non-local propagation to re-test for incon-\\nsistency in the case of a cache miss. This extra step violates the principles of dispatchable\\nexecution. Therefore, Drake maintains a complete cache of all known conﬂicts, allowing\\nDrake to verify that an environment is not known to be inconsistent with a single check\\nof the cache. Furthermore, Drake can quickly test whether any complete environments are\\nvalid, as the inconsistencies are all readily available.\\nIf there is a known conﬂict, Drake sometimes needs to determine how to avoid a conﬂict,\\nthat is, what minimal environments ensure the conﬂict is not possible.\\nDeﬁnition 5.6 (Constituent Kernels, Williams et al., 2007) A conﬂict ec has an as-\\nsociated set of constituent kernels, each of which is an environment that speciﬁes a single\\nassignment that takes some variable assigned in the conﬂict and assigns it a diﬀerent value.\\nHence, if ek is a constituent kernel, then any other environment e such that ek subsumes e\\nimplies that ec does not subsume e, and hence is not subject to that conﬂict. Thus, we say\\ne avoids the conﬂict.\\n□\\nExample 5.7 If there are three variables, X, Y, Z ∈ {1, 2}, assume {x = 1, y = 1} is a con-\\nﬂict. Then the constituent kernels are {x = 2} and {y = 2}, as any complete environment\\nthat does not contain the conﬂict must assign either x or y not to be one.\\n□\\nThe ﬁnal tool needed is the ability to perform temporal and graph reasoning on labeled\\nvalue sets, which is principally accomplished through computing path lengths or propagating\\ninequality bounds. We can construct the approach by denoting the inference rule on values\\nas a function f. Then, we can build the method for applying f to labeled value sets from\\nthe rule for applying it to labeled value pairs. We begin by deﬁning the union operation\\non environments, which is the fundamental operation on environments during temporal\\nreasoning, as in the ATMS literature (de Kleer, 1986).\\nDeﬁnition 5.8 (Union of Environments) The union of environments, denoted e ∪ e′ is\\nthe union of all the assignments of both environments. If e and e′ assign diﬀerent values\\nto some variable xi, then there is no valid union and e ∪ e′ = ⊥, where ⊥ is the symbol for\\n626\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nfalse. If e ∪ e′ is subsumed by a conﬂict, then e ∪ e′ = ⊥. This value signiﬁes that there is\\nno consistent environment where both e and e′ hold simultaneously.\\n□\\nExample 5.9 Most commonly, unions are used to compute the dependence of new derived\\nvalues. If A = 2 when {x = 1} and B = 3 when {y = 2}, then C = A + B = 5 when\\n{x = 1} ∪ {y = 2} = {x = 1, y = 2}.\\n□\\nUsing this notation, then an inference on labeled value pairs involves performing that\\ninference on the values to produce a new value, and unioning the environments to produce\\na new environment.\\nLemma 5.10 Consider labeled value pairs (a, ea), (b, eb). Applying some function f to the\\npair yields (f(a, b), ea ∪ eb).\\n□\\nProof For any environment e, if ea subsumes e then a is entailed, and if eb subsumes e then\\nb is entailed, by the deﬁnition of labeled value pairs. Additionally, ea ∪ eb subsumes e if and\\nonly if ea subsumes e and eb subsumes e. Therefore, ea ∪ eb subsumes e entails both values\\na, b, and hence allows us to compute f(a, b). Thus, (f(a, b), ea ∪ eb) is well deﬁned.\\n□\\nNote that in this lemma, ea ∪ eb may produce ⊥, which would indicate that the labeled\\nvalue pair never holds in a consistent environment and may be discarded.\\nExample 5.11 Consider computing the quantity (3, {x = 1})+(6, {y = 1}) = (3+6, {x =\\n1} ∪ {y = 1}) = (9, {x = 1, y = 1}).\\n□\\nIf the function respects the dominance of values, then we may apply it to entire labeled\\nvalue sets.\\nDeﬁnition 5.12 A function f(a, b) is consistent with ≤ dominance if for values a, b, c and\\nd, a ≤ c, b ≤ d =⇒ f(a, b) ≤ f(a, d), f(a, b) ≤ f(c, b), f(a, b) ≤ f(c, d).\\n□\\nLemma 5.13 If variables A, B are represented by labeled value sets and f is consistent with\\nthe domination ordering used for f, then the result C = f(A, B) is represented by a labeled\\nvalue set containing a non-dominated subset of the labeled value pairs (f(ai, bj), ei ∪ ej)\\nconstructed from every pair of labeled value pairs (ai, ei) ∈ A and (bj, ej) ∈ B.\\n□\\nProof The lemma follows from the argument that the cross product of applying f to all\\nthe labeled value pairs in A and B produces all possible labeled value pairs. Since we must\\nensure that the labeled value pairs include all the dominant labeled value pairs for C, we\\nrequire f to be consistent with the ordering so that the dominant values of A and B, which\\nare all we have available, are suﬃcient to derive the dominant labeled value pairs of C.\\n□\\nWe conclude this section with an example of relaxation of a labeled distance graph,\\nwhich is a core rule of inference for most weighted graph algorithms that uses the path\\nA → B → C to compute a possible path weight A → C. The derived path length replaces\\nthe old path if the newly derived path length is shorter.\\n627\\nConrad & Williams\\nExample 5.14 Consider the labeled distance graph in Figure 4.1. We compute W(A, B)+\\nW(B, C) = {(1, {})}+{(4, {x = 2})} = {(5, {x = 2})}. Compare this to the existing known\\nweights W(A, C) = {(3, {x = 1}), (7, {x = 2})}, and we determine that the new value\\nreplaces the old value of 7. After the update, W(A, C) = {(3, {x = 1}), (5, {x = 2})}.\\n□\\nThere are two fundamental inferences performed with labeled value sets: relaxation of\\nweighted edges during compilation, and sums and diﬀerences of edge weights to compute\\nbounds on execution times during dispatch. Both follow the framework outlined here, and\\nare explained in more detail in the compilation and dispatch sections, respectively. These\\noperations complete the deﬁnition of labeled value sets and the following sections use them\\nto construct compact compilation and dispatching algorithms.\\n6. Dispatching Plans with Choice\\nGiven this foundation in Labeled STNs, labeled value sets, and labeled distance graphs, we\\nturn to the central focus of this article - dynamic execution of Labeled STNs. Recall that\\nthe dispatcher uses a local, greedy algorithm to make decisions at run-time with low latency,\\nand that the accuracy of this approach is guaranteed by the compilation step. We begin\\nwith the dispatcher because low latency execution is the fundamental goal of this work and\\nbecause, as in prior work, the compiler is designed to produce an output appropriate for\\nthis dispatcher.\\nWe adapt the STN dispatcher developed by Muscettola et al.\\n(1998) to work with\\ndispatchable labeled distance graphs. Essentially, Drake’s algorithms substitute real number\\nbounds on execution times, as in the STN dispatcher, with labeled value sets. Additionally,\\nwe adapt Tsamardinos et al.’s (2001) approach to reasoning over multiple possible options,\\nthat is, allowing the dispatcher to accept a proposed execution time for an event if at least\\none full assignment to the choices is consistent with that schedule. We present Drake’s\\ndispatching algorithms by ﬁrst reviewing standard STN dispatching, and then adapt these\\ntechniques to handle labels.\\n6.1 STN Dispatching\\nMuscettola et al. (1998) showed that given a dispatchable form of the STN, a simple greedy\\ndispatcher can correctly execute the network with updates performed only on neighboring\\nevents in the dispatchable form of the STN. The dispatcher loops over the non-executed\\nevents at each time step, selecting an event to execute if possible, or else waiting until the\\nnext time step. This process continues until either all the events are executed or a failure\\nis detected.\\nDetermining whether an event is executable relies on two tests. First, the dispatcher tests\\nwhether the ordering constraints of an event have been satisﬁed, which is called testing for\\nenablement. A simple temporal constraint may imply a strict ordering between two events,\\nwhich the dispatcher must explicitly test to ensure that an event is not scheduled before an\\nevent that must precede it. Second, the dispatcher eﬃciently tracks the consequences of the\\nsimple temporal constraints between the event and its neighbors by computing execution\\nwindows for each event. Execution windows are the tightest upper and lower bounds derived\\nfor each event through one-step propagations of execution times. If the current time is in\\n628\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nan event’s execution window and it is enabled, the event may be scheduled at the current\\ntime.\\nNow we brieﬂy recall the derivation of these two rules for executing events. Recall that\\neach weighted edge in the distance graph corresponds to an inequality\\nB − A ≤ wAB\\n(11)\\nwhere A and B are execution times of events and l is some real number bound. If we\\nselect execution time tA for event A and B is not yet scheduled, then we can rearrange the\\ninequality as\\nB ≤ wAB + tA\\n(12)\\nThis produces a new upper bound for the execution time of B. Likewise, if A is not yet\\nscheduled and we select tB as the execution time of B, then we can rearrange the inequality\\nas\\nA ≥ tB − wAB.\\n(13)\\nThus, we derive a new lower bound for A. If, in this form, wAB < 0, then B < A, so event\\nB must precede A, implying an enablement constraint.\\nWe can recast these rules in terms of propagations on the distance graph. If an event\\nA is scheduled at time t, then propagate it through all outbound edges (A, B) to derive\\nupper bounds B ≤ wAB + t, and through all inbound edges (B, A) to derive lower bounds\\nB ≥ t − wBA. Event B has event A as a predecessor if there is a negative weight edge\\n(B, A). As usual, dispatching is only aﬀected by the dominant upper and lower bounds, so\\nthe dispatcher only stores the dominant constraints.\\nExample 6.1 Consider the dispatchable distance graph fragment in Figure 6.1. The exe-\\ncution windows begin without constraint, −∞ ≤ A ≤ ∞ and −∞ ≤ B ≤ ∞. If we begin\\nexecution at t = 0, B is not executable yet because it is not enabled; the negative weighted\\nedge (B, A) implies that A must be executed ﬁrst. A has no predecessor constraints and\\nzero lies within its execution bound, so may be executed at this time. Propagating the time\\nA = 0 allows us to derive the bounds 2 ≤ B ≤ 8. Then B may be executed at any time\\nbetween 2 and 8. If the dispatcher reached time 9 without having executed B, then it must\\nsignal a failure.\\n□\\nA\\nB\\n-2\\n8\\nFigure 6.1: A dispatchable distance graph fragment.\\nThe ﬁnal consideration for the dispatcher is zero-related events. If two events are con-\\nstrained to be executed at precisely the same time, then the prior work requires that the\\nevents are collapsed into a single vertex in the dispatchable graph. Otherwise, zero-related\\nvertices may cause the dispatcher to make mistakes because they must occur together, yet\\n629\\nConrad & Williams\\nthey appear independently schedulable. Equivalently, the dispatcher may simulate the col-\\nlapse by always executing zero-related vertices as a set. For example, if A and B are known\\nto be zero-related, the dispatcher may schedule them together, scheduling the events at time\\nt if the execution windows and enablement constraints of both A and B are satisﬁed. Note\\nthat since they are zero-related, it is impossible to have an enablement constraint between\\nthem.\\n6.2 Labeled STN Dispatching\\nDrake relies on these same fundamental structures and rules as the STN dispatcher, but\\nmodiﬁes each step to consider labels. Since edge weights are labeled value sets, the execution\\nwindows are also labeled value sets, implying that the upper and lower bounds of execution\\ntimes for events may vary depending on the assignments to choices and may vary separately.\\nFor each possible bound or enablement constraint, Drake’s dispatcher must either enforce\\nthe constraint, or discard the constraint and decide not to select the associated environment.\\nBroadly, this is the same strategy as Tsamardinos et al. (2001), where the component STNs\\nare dispatched in parallel, and proposed scheduling decisions may be accepted if they are\\nconsistent with at least one STN.\\nWe begin by considering how to update the propagation rules to derive execution win-\\ndows.\\nThe STN propagations involve adding edge weights (or negative weights) to the\\nexecution time of an event.\\nThe upper bound of every event is initially {(∞, {})} and\\nevery lower bound is initially {(−∞, {})}. Upper bounds are dominated by low values,\\nor the ≤ inequality, and lower bounds are dominated by large values, or the ≥ inequality.\\nThe following theorem describes the propagation of execution bounds, which is the labeled\\nimplementation of Equations 12 and 13.\\nTheorem 6.2 If event A is executed at time t, then consider some other event B. For\\nevery (wAB, eAB) ∈ W(A, B), (wAB + t, eAB) is a valid upper bound for execution times\\nof B and for every (wBA, eBA) ∈ W(B, A), (t − wBA, eBA) is a lower bound for execution\\ntimes of B.\\n□\\nProof These rules are a direct extension of the STN propagation to the labeled case using\\nlabeled operations as in Deﬁnition 5.12. Since the execution actually occurred, it holds\\nunder all possible environments, thus we give it the empty environment {}. Then we apply\\nDeﬁnition 5.12, substituting the labeled version of addition.\\n□\\nAt dispatch, these new bounds are added to the labeled value sets Bu and Bl, which\\nmaintain the non-dominated bounds.\\nA\\nB\\n{(−5, {x = 1}), (−2, {})}\\n{(7, {x = 1}), (8, {})}\\nFigure 6.2: A dispatchable labeled distance graph fragment.\\n630\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nExample 6.3 Consider the labeled distance graph fragment in Figure 6.2.\\nIf event A\\nis executed at t = 2, then we derive bounds {(7, {x = 1}), (4, {})} ≤ B ≤ {(9, {x =\\n1}), (10, {})}\\n□\\nSince the bounds may vary between choices, Drake cannot generally expect to obey all\\nthe possible constraints, but instead is only required to enforce all the constraints implied\\nby at least one complete environment.\\nExample 6.4 Assume an event has lower bounds represented by the labeled value set\\nA ≥ {(2, {x = 1}), (0, {})}. This implies that under any set of choices where x = 1, A ≥ 2,\\nand otherwise, A ≥ 0 is suﬃcient. If the dispatcher executes A at t = 0, then it may not\\nselect x = 1. Thus, if the dispatcher can restrict its choices to those that do not include\\nx = 1, while leaving other consistent options, then it may execute A at t = 0. If all the\\nremaining consistent full assignments to the choices require that x = 1, then the dispatcher\\nmust wait until t = 2 before executing A.\\n□\\nDrake performs this reasoning by collecting the environments for the bounds that a\\nparticular execution would violate, and by determining whether those environments can\\nbe made conﬂicts without making every complete environment inconsistent. If complete\\nenvironments would remain consistent, the execution is performed and then environments\\nare made conﬂicts; otherwise the execution is not possible, and discarded.\\nFinally, Drake simulates the collapse of zero-related events. At each time, Drake at-\\ntempts to execute each event individually and also attempts to execute the sets of zero-\\nrelated vertices recorded by the compiler. As noted before, zero-related sets may exist under\\nsome environments and not in others. If the zero-related set is enforced, then all of its mem-\\nber events must be executed together, or not at all. Therefore, if the dispatcher considers\\nexecuting a set of events that is a strict subset of a particular zero-related set, leaving out at\\nleast one other member of the zero-related set, then it must discard the environments where\\nthe zero-related set is implied. Additionally, while there cannot be enablement constraints\\nbetween zero-related events under any complete environments where the zero-related group\\nholds, other environments may imply strict orderings between the events, which the dis-\\npatcher discards when executing the zero-group simultaneously. Hence, the dispatcher must\\ndiscard any associated environments.\\nWe summarize the rules as follows, which are further illustrated in Example 6.7.\\nTheorem 6.5 If S is set with one event or a set of zero-related events in a Labeled STN,\\nthen the set may be executed at time t if and only if there is at least one consistent complete\\nenvironment where:\\n1. For every Al that is the lower bound labeled value set of some A ∈ S, such that Al ≤ A,\\nfor every (l, e) ∈ Al such that l > t, e is a conﬂict.\\n2. For every Au that is the upper bound labeled value set of some A ∈ S, such that\\nAu ≥ A, for every (u, e) ∈ Au such that u < t, e is a conﬂict.\\n3. For every pair of events, A ∈ S, B /∈ S, such that B is not yet scheduled, for every\\n(w, e) ∈ W(A, B) such that w < 0, e is a conﬂict.\\n631\\nConrad & Williams\\n4. For every zero-related set of events Z with environment e, if S ⊂ Z, e is a conﬂict.\\n5. For every A1, A2 ∈ S, for every (w, e) ∈ W(A1, A2) such that w < 0, e is a conﬂict.□\\nProof If there is a consistent full environment after creating these conﬂicts, then it is\\nnecessarily not subsumed by the environments of any of the constraints that imply the\\nevents cannot be executed at time t. Thus, the consistent full environment corresponds to a\\ncomponent STN where every constraint holds, so the events may be executed. If there is no\\nconsistent environment, then at least one of the above constraints prohibiting the proposed\\nexecution exists in each remaining component STN, so the execution is not valid.\\n□\\nSimilarly to the STN case, the dispatcher must check for missed upper bounds during\\nevery time step. In an STN, a missed upper bound implies the execution has failed. In a\\nLabeled STN, a missed upper bounds implies that any complete environments subsumed\\nby the environment of the missed upper bound are no longer valid.\\nTheorem 6.6 If event A is not executed at time t, has upper bound labeled value set Au,\\nand (u, e) ∈ Au such that u < t, then e is a conﬂict.\\n□\\nProof The theorem follows from noting that the upper bound exists in every component\\nSTN corresponding to a complete environment subsumed by e, thus every environment\\nsubsumed by e is inconsistent, which by deﬁnition, makes e a conﬂict.\\n□\\nIt is possible that missed upper bounds could invalidate all remaining complete en-\\nvironments, in which case dispatch has failed and the dispatcher should signal an error\\nimmediately.\\nExample 6.7 Consider the dispatchable labeled distance graph in Figure 6.3, with events\\nA, B, C, D and choice variables x, y ∈ {1, 2}.\\nThere is a zero-related set {A, C} under\\nenvironment {x = 1}. Assume that the current time is t = 0 and no events have been\\nexecuted yet.\\nAll the possible complete environments are initially consistent.\\nConsider\\nsome possible executions and their consequences.\\n• Event C has no predecessors, and no restrictions from its execution window. However,\\nit is part of the zero-related set, so it may only be executed if we can make {x = 1}\\na conﬂict, in order to remove the zero-related set from all possible executions. This\\nis feasible, so we may execute C at t = 0 and create the conﬂict.\\nIf we do this,\\nthen the ordering and inequality implied by edge (A, D) is necessary for a consistent\\nexecution, as its environment cannot be made a conﬂict without making all complete\\nenvironments inconsistent.\\n• Event A has D as a predecessor under {x = 2}, C as a predecessor under {y = 1},\\nand is part of the zero-related set under {x = 1}. To execute A at t = 0, we would\\nneed to make all three environments conﬂicts, but that would invalidate all possible\\nchoices, so we may not execute A at t = 0.\\n632\\nDrake: An Efficient Executive for Temporal Plans with Choice\\n• The zero-related set {A, C} has no execution window restrictions from either A or\\nC, and has a predecessor D under {x = 2} from the edge (A, D). Additionally, edge\\n(A, C) has a negative weight −1 with environment {y = 2}, so that environment is\\nalso made a conﬂict. Thus, we can create the conﬂicts {x = 2} and {y = 2} and\\nexecute both A and B at t = 0.\\n• Event B has A as a predecessor under {y = 1} and A is not yet executed, so we may\\nmake that environment a conﬂict and execute B. Assume that A and C are executed\\nat t = 0, then the enablement constraint is satisﬁed, but any execution of B before\\nt = 3 requires making {y = 1} a conﬂict. Additionally, if the current time grows to\\nt = 7 and B has not yet been executed, then the upper bound has been violated and\\n{y = 1} is then a conﬂict.\\n• Event D has predecessor C under environment {}, but C has not yet been executed.\\nSince making {} a conﬂict makes all complete environments inconsistent, D cannot\\nbe executed.\\n□\\nA\\nB\\nC\\nD\\n{(0, {x = 1}),\\n(−1, {y = 1})}\\n{(0, {x = 1})}\\n{(6, {y = 1})}\\n{(−3, {y = 1})}\\n{(6, {y = 2})}\\n{(−1, {})}\\n{(−2, {x = 2})}\\nFigure 6.3: A dispatchable labeled distance graph.\\nThis completes our presentation of Drake’s dispatch algorithms. Essentially, it makes\\ntwo adaptations to the STN dispatcher: (1) maintain labeled value sets for the execution\\nwindows of events and (2) allow the dispatcher to select between the choices by creat-\\ning conﬂicts. The next section describes the compilation algorithm, which computes the\\ndispatchable form of input Labeled STNs, ensuring that the local reasoning steps in the\\ndispatching algorithm will satisfy the requirements of the plan.\\n633\\nConrad & Williams\\n7. Compiling Labeled Distance Graphs\\nWe complete the description of Drake with the compiler, which reformulates the input La-\\nbeled STN into a form the dispatcher is guaranteed to execute correctly. The compiler\\nleverages all of the labeling concepts we presented to eﬃciently compute a compact dis-\\npatchable form of input plans. An STN compiler takes a distance graph as its input and\\noutputs another distance graph, the minimal dispatchable form of the input problem. Sim-\\nilarly, Drake’s compiler takes a labeled distance graph as its input and outputs a labeled\\ndistance graph that is the minimal dispatchable form of the input.\\nMuscettola et al.\\n(1998) introduced an initial compilation algorithm for STNs that\\noperates in two steps. First, it computes the All-Pairs Shortest Path (APSP) graph asso-\\nciated with the STN, which is itself a dispatchable form. Second, the compiler prunes any\\nedges that it can prove are redundant to other non-pruned edges, and which the dispatcher\\ntherefore does not need to make correct decisions. The pruned edges are dominated, and\\ntheir removal signiﬁcantly reduces the size of the dispatchable graph. The compiler tests\\nfor dominance by applying the following rule on every triangle in the APSP graph.\\nTheorem 7.1 (Triangle Rule, Muscettola et al., 1998) Consider a consistent STN\\nwhere the associated distance graph satisﬁes the triangle inequality; a directed graph sat-\\nisﬁes the triangle inequality if every triple of vertices (A, B, C) satisﬁes the inequality\\nW(A, B) + W(B, C) ≥ W(A, C).\\n(1) A non-negative edge (A, C) is upper-dominated by another non-negative edge (B, C)\\nif and only if W(A, B) + W(B, C) = W(A, C).\\n(2) A negative edge (A, C) is lower-dominated by another negative edge (A, B) if and\\nonly if W(A, B) + W(B, C) = W(A, C).\\n□\\nAlthough the basic concept of domination is unchanged, in that dominated constraints\\nare not needed by the executive, the speciﬁcs are quite diﬀerent here. Domination within a\\nlabeled value sets refers to labeled values that make other labeled values within that same\\nset unnecessary. In the context of edge pruning, one edge could dominate another if the\\ntwo edges share a start or end vertex, but not both. When we develop the labeled version\\nof edge pruning, the labeled value from one edge weight labeled value set can dominate a\\nvalue from a diﬀerent edge weight labeled value set, where the corresponding edges share a\\nstart or end vertex.\\nExample 7.2 Consider the two weighted graph fragments in Figure 7.1. In each case, edge\\n(A, C) is dominated, as shown by the theorem.\\n□\\nThe compiler searches for all dominated edges and then removes them all together. Some\\ncare is necessary to ensure that a pair of edges AC and BC do not provide justiﬁcation\\nfor pruning each other; these edges are said to be mutually dominant. Typically, pruned\\ngraphs have a number of edges closer to O\\n�\\nN log N\\n�\\n, than N2, a signiﬁcant savings. As a\\nﬁrst prototype of Drake, Conrad, Shah, and Williams (2009) introduced a labeled extension\\nof this algorithm. This extension was used to perform task execution on the ATHLETE\\nRover within the Mars Yard at NASA JPL.\\nAlthough simple and eﬀective, this algorithm does not scale gracefully to large problems\\nbecause it uses the entire APSP graph an intermediate representation, which is much larger\\n634\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nA\\nB\\nC\\n-5\\n-4\\n1\\n(a)\\nLower\\ndomination\\nweighted graph\\nA\\nB\\nC\\n2\\n5\\n3\\n(b)\\nUpper\\ndomination\\nweighted graph\\nFigure 7.1: Examples of upper and lower dominated edges.\\nthan the ﬁnal result. To resolve this, Tsamardinos, Muscettola, and Morris (1998) presented\\na fast compilation algorithm that interleaves the APSP step and the pruning step, and avoids\\never storing the entire APSP graph. Their algorithm is derived from Johnson’s algorithm for\\ncomputing the APSP, which incrementally builds the APSP by repeated SSSP computations\\n(Cormen, Leiserson, Rivest, & Stein, 2001). Johnson’s algorithm uses Dijkstra’s algorithm\\nas an inner loop, providing it with faster performance than Floyd-Warshall for sparse graphs.\\nEssentially, the fast compilation algorithm loops over the events in the graph, computes the\\nSSSP for that event, and adds all non-dominated edges with the event as the source to the\\ndispatchable form. Thus, this algorithm only needs to store the ﬁnal dispatchable graph\\nand one SSSP, avoiding the bloat in the intermediate representation. Thus, this algorithm\\nhas better space and time complexity than the APSP step followed by pruning.\\nThis paper introduces a novel extension to the Drake system that adapts the fast com-\\npilation algorithm to labeled graphs in order to avoid unnecessary storage growth during\\ncompilation. The fast algorithm requires a number of steps; this section describes each\\ncomponent of the original fast algorithm and its adaptation to the labeled setting. First,\\nwe recall Johnson’s strategy to compute the APSP through iterated SSSP, and present\\nthe labeled adaptation of the SSSP algorithm. Second, we discuss the predecessor graphs\\nthat result from the SSSP and how to traverse them. Third, we describe how to interleave\\npruning with the repeated SSSP computations. Finally, we discuss the issues arising from\\nmutual dominance, and the preprocessing step used to resolve them.\\n7.1 Johnson’s Algorithm and the Structure of the Fast Algorithm\\nMany shortest path algorithms on weighted distance graphs essentially perform repeated\\nrelaxation over the graph. Floyd-Warshall loops repeatedly over the entire graph, relaxing\\nthe edges and computing the entire APSP in a single computation. Johnson’s algorithm\\ncomputes the APSP one source vertex at a time, using Dijkstra’s SSSP algorithm as an\\ninner loop to perform relaxations more eﬃciently. Since Dijkstra’s algorithm only works\\nfor positively weighted graphs, Johnson’s algorithm re-weights the graph before Dijkstra\\ncalls and un-weights it afterward. Unfortunately, adapting Dijkstra’s algorithm to labeled\\ndistance graphs is ineﬃcient because re-weighting both adds and subtracts labeled value\\nsets, and both are not compatible with a single ordering.\\nTherefore, we use Bellman-\\nFord’s SSSP algorithm instead; we sacriﬁce the improved run-time of the fast algorithm,\\n635\\nConrad & Williams\\nbut preserve the lower space overhead. The fast STN compilation algorithm copies the\\nessential structure of Johnson’s algorithm. This section illustrates the overall algorithm in\\nthe unlabeled case with a partial example.\\nExample 7.3 Consider one step of compilation shown in Figure 7.2. The input distance\\ngraph is shown in Figure 7.2a. When we compute the SSSP of the input for source vertex\\nB, the output is the predecessor graph shown in 7.2b, which depicts the shortest distances\\nand paths from B. The weights on the vertices indicate that the complete APSP graph\\nshould contain an edge from B to A with weight −4, B to C with weight −1, and B to D\\nwith weight 0. Furthermore, in the original graph, the shortest path from B to A uses the\\nedge B → A. The shortest path from B to C is either B → C or B → A → C; both have\\nequal length. The shortest path from B to D is B → D.\\nHowever, the dispatchable form of this problem does not actually need all three of\\nthese edges, which the compiler can deduce from the predecessor graph and SSSP values.\\nRoughly, since A has a lower distance from B than C, and is along a shortest path to C, the\\nedge BC is not necessary. Therefore, the other two edges are inserted in the dispatchable\\ngraph, depicted in Figure 7.2c, and the edge BC is discarded. This process is repeated for\\nthe other three vertices.\\n□\\nA\\nB\\nC\\nD\\n8\\n5\\n3\\n-4\\n-1\\n0\\n-1\\n3\\n1\\n(a) Input Distance Graph\\nA\\n−4\\nB\\nC\\n−1\\nD\\n1\\n(b) Predecessor Graph for Source\\nB\\nA\\nB\\nC\\nD\\n-4\\n0\\n(c)\\nPartial\\ndispatchable\\ngraph with edges that have\\nsource vertex B\\nFigure 7.2: A single step of the fast reformulation algorithm for STNs\\n7.2 Labeled Bellman-Ford Algorithm\\nDrake uses the Bellman-Ford algorithm as a central building block for the variant of the fast\\nSTN algorithm Drake uses; Bellman-Ford derives the tightest possible edge weights, while\\nsimultaneously deriving the predecessor graph. This graph provides enough information\\nto prune the dominated edges. To begin, we provide Algorithm 7.1, taken directly (with\\nslightly altered notation) from the work of Cormen et al. (2001). The algorithm loops over\\nthe edges and performs relaxations, then tests for negative cycles to ensure the result is\\nvalid. The value d[v] is the distance of the vertex v from the input source vertex s. The\\nvalue π[v] is the vertex that is the (not necessarily unique) predecessor of v when forming\\n636\\nDrake: An Efficient Executive for Temporal Plans with Choice\\na shortest path from s to v. Relating to Figure 7.2b, d corresponds to the annotations next\\nto the vertices, and π speciﬁes the directed edges.\\nAlgorithm 7.1 Bellman-Ford Algorithm\\n1: procedure BellmanFord(V, E, W, s)\\n2:\\nInitializeSingleSource(V, W, s)\\n3:\\nfor i ∈ {1...|V | − 1} do\\n▷ Loop for relaxation\\n4:\\nfor each edge (u, v) ∈ E do\\n5:\\nRelax(u, v, W)\\n6:\\nend for\\n7:\\nend for\\n8:\\nfor each edge (u, v) ∈ E do\\n▷ Check for negative cycles\\n9:\\nif d[v] > d[u] + W(u, v) then\\n10:\\nreturn false\\n▷ Fail if a negative cycle is found\\n11:\\nend if\\n12:\\nend for\\n13:\\nreturn true\\n14: end procedure\\n15: procedure InitializeSingleSource(V, s)\\n16:\\nfor each vertex v ∈ V do\\n17:\\nd[v] ← ∞\\n18:\\nπ[v] ← nil\\n19:\\nend for\\n20:\\nd[s] ← 0\\n21: end procedure\\n22: procedure Relax(u, v, W)\\n23:\\nif d[v] > d[u] + W(u, v) then\\n24:\\nd[v] ← d[u] + W(u, v)\\n25:\\nπ[v] ← u\\n26:\\nend if\\n27: end procedure\\nAs we will see in the next few sections, the fast compilation algorithm for STNs requires\\nthat we have access to the entire predecessor graph, which encodes all shortest paths in the\\ngraph, rather than any single path. To compute the entire graph, Drake makes each π[v] a\\nset, and if Relax determines that d[v] = d[u] + W(u, v), it then pushes u onto d[v]. When\\nthe strict inequality holds and d[v] is updated, π[v] is set to the single element set {u}.\\nNow we discuss the changes necessary to adapt the Bellman-Ford algorithm to Labeled\\nDistance Graphs. The relaxation procedure computes the new bound on the path length\\nfrom s to v, and if it dominates the old value, replaces the old value. If the vertex u allows\\nus to derive the current dominant value d[v], then it is stored in π[v]. Drake’s use of labeled\\nvalue sets that store value pairs (d, π) ∈ R × V allow it to perform both tasks at once, with\\none data structure.\\n637\\nConrad & Williams\\nTo see how this works, consider an example:\\nExample 7.4 In Figure 7.3a, edge (A, B) forms a snippet of a labeled distance graph;\\nconsider computing the SSSP with source vertex A. We know that the initialization routine\\nunder every choice is to set d[u] = 0 for the source vertex and d[u] = ∞ for all others, and\\nto set π[u] = nil for every vertex. In the combined notation, the initial value for the weight\\nof the vertices is (0, nil) or (∞, nil), labeled with the empty environment {}. These are\\nthe initial weights shown on the vertices.\\nThe relaxation of this edge allows us to derive that B is distance 8 from A under environ-\\nment {x = 1}, and has predecessor A. Hence, we add the labeled value pair ((8, A), {x = 1})\\nto the labeled value set for the weight of B. This new value has a tighter value for d, and\\nhence is kept, but does not dominate the old value, thus both are kept. This single insertion\\nstep updates both the distance of B from A and the predecessor of B along the shortest\\npath. The result is depicted in Figure 7.3b.\\n□\\nIt should now be clear why we elected to keep non-identical values with the same numeric\\nvalue: the labeled value sets can hold all the predecessors for the target vertices, and not just\\na single predecessor. Algorithm 7.2 shows the generalization of this example. Initialization\\nis exactly as in the example, it assigns the initial values from the standard algorithm with\\nempty environments. During relaxation, following Lemma 5.13, the potential new path\\nlengths are the cross product addition of the weight of u added to the weight of the edge\\nfrom u to v, labeled with the union of the environments, if the union is valid and not\\nsubsumed by any known conﬂicts. Naturally, when relaxing u through an edge to v, u\\nis the predecessor. When adding this value to the labeled value set d[v], the domination\\ncriterion is used to determine if this new path length is non-dominated, and to prune non-\\ndominant path lengths and predecessors as appropriate.\\nThe ﬁnal modiﬁcation of the original algorithm is in the test for negative cycles; a\\nnegative cycle is detected if another relaxation of any edge would further decrease the\\ndistance from the source to any other vertex. In the labeled case, some choices may have\\nnegative cycles and others may not. We begin the computation by computing the relaxation\\nd[u]+W(u, v) with the labeled addition operator, and discard the predecessor vertices from\\nd[u], thus producing a real valued labeled value set. If there is any environment where\\nd[v] > d[u] + W(u, v) holds, that environment is a conﬂict of the Labeled STN.\\nFinding the minimal conﬂicts in practice is somewhat convoluted. Consider each pair\\nof labeled values (dv, ev) ∈ d[v], (duw, euw) ∈ d[u] + W(u, v) in turn. If dv > duw then there\\nmight be a conﬂict, however, there may be some environments where a smaller, dominant\\nvalue of d[v] takes precedence over dv, thus preventing the inequality from being satisﬁed.\\nHence, if it exists, the conﬂict we deduce is ev ∪ euw after modifying the union to avoid any\\nother environments e′\\nv such that (d′\\nv, e′\\nv) ∈ d[v] and d′\\nv ≤ duw. It is possible that conﬂicts\\ncould invalidate all possible choices, which the executive tests for, before determining that\\nthe plan is dispatchable.\\n638\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nExample 7.5 Consider the following values for the inequality test, given two binary choice\\nvariables:\\nd[v] = {(2, {y = 1}), (4, {})}\\n(14)\\nd[u] + W(u, v) = {(3, {x = 1}), (5, {})}\\n(15)\\n(16)\\nWe must ﬁnd and make conﬂicts for any minimal environments that imply d[v] > d[u]+\\nW(u, v) holds.\\nEach side of the inequality has two possible values, so we can consider\\nfour pairs of values that could satisfy the inequality. Three of the pairs do not satisfy the\\ninequality, and thus can cause no conﬂict: (2, {y = 1}) < (3, {x = 1}), (2, {y = 1}) < (5, {})\\nand (4, {}) < (5, {}).\\nThe last pair, (4, {}) > (3, {x = 1}), satisﬁes the inequality, so\\nthis pair could imply that the union of corresponding environments, {} ∪ {x = 1}, is a\\nconﬂict. However, to reach (4, {}) for the left hand side, we skipped over a smaller value,\\n(2, {y = 1}) ∈ d[v]. To correctly address this ordering, the conﬂict must also avoid the\\nenvironment for the smaller value. Taking {} ∪ {x = 1} and avoiding {y = 1} produces one\\nenvironment, {x = 1, y = 2}, which is a valid environment, and hence it is a conﬂict.\\n□\\nA\\n{((0 , nil) ,{})}\\nB\\n{((∞ , nil) ,{})}\\n{(8, {x = 1})}\\n(a) Before Relaxation\\n{((0, nil), {})}\\n{((8, A), {x = 1}), ((∞, nil), {})}\\nA\\nB\\n{(8, {x = 1})}\\n(b) After Relaxation\\nFigure 7.3: A single labeled relaxation step.\\n7.3 Traversals of Labeled Predecessor Graphs\\nThe fast compilation algorithm for STNs performs traversals of the predecessor graphs\\nproduced by the SSSP analysis, and checks each edge along the traversal for dominance.\\nThis is used to reduce the graph to minimal dispatchable form. In the unlabeled case, the\\nπ[u] values specify a directed graph such as in Figure 7.2b, where we may use depth-ﬁrst\\nexploration to enumerate all possible paths beginning with the source vertex the SSSP is\\ncomputed for. However, in the labeled case, more care is necessary.\\nThe simplest way to understand which paths are valid under complete environments is\\nto consider the projection of the predecessor graph under a particular complete environment\\ne. Take every labeled value set d[u] and query it under e. The result gives the shortest path\\ndistance and any predecessors, producing a standard unlabeled predecessor graph.\\n639\\nConrad & Williams\\nAlgorithm 7.2 Labeled Bellman-Ford Algorithm\\n1: procedure LabeledBellmanFord(V, E, W, S, s)\\n2:\\nLabeledInitializeSingleSource(V, W, s)\\n3:\\nfor i ∈ {1...|V | − 1} do\\n▷ Loop over relaxations\\n4:\\nfor each edge (u, v) ∈ E do\\n5:\\nLabeledRelax(u, v, W)\\n6:\\nend for\\n7:\\nend for\\n8:\\nfor each edge (u, v) ∈ E do\\n▷ Test for negative cycles\\n9:\\nfor each labeled value pair (dv, ev) ∈ d[v] do\\n10:\\nfor each labeled value pair (duw, euw) ∈ d[u] + W(u, v) do\\n11:\\nif dv > duw then\\n12:\\nAddConflict(ev ∪ euw, split to avoid all e′\\nv such that\\n13:\\n(d′\\nv, e′\\nv) ∈ d[v] and d′\\nv ≤ duw)\\n14:\\nend if\\n15:\\nend for\\n16:\\nend for\\n17:\\nend for\\n18:\\nIsSomeCompleteEnvironmentConsistent?()\\n19: end procedure\\n20: procedure LabeledInitializeSingleSource(V, s)\\n21:\\nfor each vertex v ∈ V \\\\ s do\\n22:\\nd[v] ← {((∞, nil), {})}\\n23:\\nend for\\n24:\\nd[s] ← {((0, nil), {})}\\n25: end procedure\\n26: procedure LabeledRelax(u, v, W)\\n27:\\nfor labeled value pair ((du, πu), eu) ∈ d[u] do\\n28:\\nfor labeled value pair ((dw, πw), ew) ∈ W(u, v) do\\n29:\\nAddToLVS(d[v], ((du + dw, u), eu ∪ ew))\\n30:\\nend for\\n31:\\nend for\\n32: end procedure\\n640\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nExample 7.6 Consider the labeled graph fragment in Figure 7.4a, where the vertices are\\nlabeled with their SSSP weights for source vertex A.\\nFurther consider the predecessor\\ngraph and path lengths implied under environment e = {x = 1, y = 1}: d[B](e) = (2, A)\\nand d[C](e) = (1, A). Hence, both vertices B and C have A as their only predecessor, and\\nB is distance 2 from A, and C is distance 1 from A, as shown in Figure 7.4b. Vertex D is\\ndistance 6 from A, with predecessor C. An important feature of this example is that for\\nthis environment, C is not a predecessor of B, even though the pair (7, C) is labeled with\\n{y = 1}, which subsumes e. Predecessors are only provided by the dominant path length.□\\n{((0, nil), {})}\\n{((2, A), {x = 1, y = 1}),\\n((7, C), {y = 1}),\\n((∞, nil), {})}\\nA\\nB\\nC\\nD\\n{((1, A), {x\\n=\\n1}),\\n((3, A), {})}\\n{((6, C), {x = 1, y = 1}),\\n((8, C), {y\\n=\\n1}),\\n((∞, nil), {})}\\n{(2, {x = 1, y = 1})}\\n{(1, {x = 1}), (3, {})}\\n{(4, {y = 1})}\\n{(5, {y = 1})}\\n(a) Relaxed Graph\\nA\\n0\\nB\\n2\\nC\\n1\\nD\\n6\\n(b) Predecessor graph for\\ne = {x = 1, y = 1}\\nFigure 7.4: Extracting predecessor graphs from labeled SSSP\\nAlthough it is instructive to consider the projection of the SSSP onto an environment for\\nintuitive understanding, an implementation based on this approach is not eﬃcient in space\\nor time. As in the rest of this work, Drake directly performs the traversals on the labeled\\nrepresentation. Since paths in the predecessor graphs may exist under some environments\\nand not others, we take the natural step and use minimal environments to specify when a\\n641\\nConrad & Williams\\nparticular path exists. Intuitively, we expect that the environment of a path is simply the\\nunion of the environments of the labeled value pairs that speciﬁed the predecessor edges\\nused. This is true, but we require an extra step to test the validity of the path for that\\nenvironment.\\nConsider a partial path from the source vertex S, which passes through some subset of\\nvertices X1 → . . . Xn, constructed from labeled value pairs ((di, Xi), ei) ∈ d[Xi+1], which\\nrepresent some path through the predecessor graph. Then the path is valid if, for every\\nvertex along the path, (di, Xi) ∈ d[Xi+1](e1 ∪ . . . en), and the path environment is e =\\ne1 ∪ . . . en. Essentially, this tests that labeled value pairs used to construct the path are\\nactually still entailed by the union of all of their respective environments.\\nIf a path is\\ninvalid, all possible extensions are also invalid, because the same inconsistency will exist in\\nall extensions.\\nExample 7.7 We can apply this criteria to determine all the paths given by the SSSP in\\nFigure 7.4a. Begin with the source vertex, A, which clearly exists under all environments,\\nso we assign this partial path the empty environment. B contains the labeled value pair\\n((2, A), {x = 1, y = 1}), and thus is a candidate to extend our path. The union of the two\\nenvironments is {}∪{x = 1, y = 1} = {x = 1, y = 1}. When queried with this environment,\\nd[B] returns the labeled value pair that we used to propose the extension, so the path\\nA → B has environment {x = 1, y = 1}. B has no other possible extensions.\\nReturning to A, it has a possible extension to C because of the labeled value pairs\\n((1, A), {x = 1}) and ((3, A), {}). First, consider ((1, A), {x = 1}), and the query of d[C]\\nunder {x = 1} shows that the path exists, so we have the partial path A → C with\\nenvironment {x = 1}. This path has a potential extension to B because of the labeled\\nvalue pair ((8, C), {y = 1}). However, the query d[B]({x = 1} ∪ {y = 1}) = (2, A) ̸= (7, C),\\nso the extension is not valid, as expected from our earlier discussion. We may instead extend\\nthis path to D, giving the path A → C → D the environment {x = 1, y = 1}. The queries\\non both d[C] and d[D] give the values used to generate the path, and hence the path is\\nvalid.\\nReturning to A, we consider the other path to B, using the labeled value pair ((3, A), {}).\\nNow the extension to B passes the query test, so A → C → B provides path length 7 if\\n{x = 1} but not {y = 1}. We label this path with the label {x = 1}, but must take care in\\nlater algorithms not to use it to imply that 7 is the shortest path length for {x = 1, y = 1}.\\nFinally, we can extend the path to D, giving path A → B → D with length 8 under {y = 1},\\nwith the same caveat.\\n□\\nGiven this rule specifying the correct extensions to partial paths, we can construct a\\ndepth-ﬁrst search to enumerate all possible paths in the acyclic predecessor graph, re-testing\\nat each step to ensure the path is valid.\\n7.4 Pruning Dominated Edges with the SSSP\\nWith the strategy for ﬁnding labeled paths through the labeled predecessor graphs, the\\nextension of the pruning algorithm from the fast STN compilation algorithm is straight-\\nforward. Tsamardinos, et al. (1998) provide two theorems relating dominance of edges to\\npaths in the predecessor graph, adjusted slightly for our notation. In the following, A is\\nassumed to be the source vertex of the SSSP, and B and C are other vertices.\\n642\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nTheorem 7.8 A negative edge (A, C) is lower-dominated by a negative edge (A, B) if and\\nonly if there is a path from B to C in the predecessor graph for A.\\n□\\nTheorem 7.9 A non-negative edge (A, C) is upper-dominated if and only if there is a\\nvertex B, distinct from A and C, such that d[B] ≤ d[C] and there is a path from B to C in\\nthe predecessor graph for A.\\n□\\nExample 7.10 Figure 7.5 shows two simple examples on which we can apply these two\\ntheorems. First, Figure 7.5a is a weighted distance graph and Figure 7.5c shows its pre-\\ndecessor graph for source vertex A, on which we apply Theorem 7.8. Edge (A, C) is lower\\ndominated because the weight −5 on B implies an edge (A, B) with weight −5, which is\\nnegative as the theorem requires. Furthermore, the predecessor graph has a path from B\\nto C in the predecessor graph for A. Therefore, edge (A, C) is dominated and needed in\\nthe dispatchable form.\\nFigures 7.5a and 7.5c similarly exhibit Theorem 7.9. There is a path A → B → C in\\nthe graph, and d[B] = 2 ≤ d[C] = 5, so edge (A, C) is upper-dominated. Thus, edge (A, C)\\nis not needed in the dispatchable form.\\nIn both these examples, this step derives every possible edge weight for edges in the\\ndispatchable form with A as their source, namely edges (A, B) and (A, C), and determines\\nthat only (A, B) is actually needed.\\n□\\nIn the labeled case, particular labeled edge weights are dominated if the above conditions\\nhold under all the environments the weight holds in.\\nTheorem 7.11 A negative labeled edge weight (dC, eC) in d[C] is lower-dominated by a\\nnegative labeled edge weight (dB, eB) in d[B] if and only if eB = eC and there is a path from\\nB to C with environment eP in the predecessor graph for A such that eP = eC.\\n□\\nTheorem 7.12 A non-negative labeled edge weight (dC, eC) in edge d[C] is upper-dominated\\nif and only if there is a labeled edge weight (dB, eB) in d[B], such that B is distinct from A\\nand C, eB subsumes eC, dB ≤ d, C, and there is a path from B to C with environment eP\\nin the predecessor graph for A such that eP = eC.\\n□\\nIn practice, Drake searches over every path in the labeled predecessor graph with the\\nsource vertex as its start, and applies these theorems to ﬁnd dominated edges. Speciﬁcally,\\nduring the traversal, it records the smallest vertex weight of any vertices along the path,\\nnot counting the source. That value is compared to other vertex weights of extensions of\\nthe path to apply the domination theorems. Every time a vertex weight is found to be\\ndominated with some path, it is recorded in a list. After all traversals are done, every\\nlabeled value in the vertex weights not present in the list of dominated values is converted\\ninto an edge in the output dispatchable graph.\\nThese two theorems require that eP = eC because the path must hold under all envi-\\nronments where the value dC does, but we also do not want eP to be tighter. Recall that\\nthe vertex weights we might prune also specify the paths. If eC is tighter than eP , then it\\nmust have a lower path length than the one implied by eP , or else it would not be in the\\nlabeled value set d[C]. Thus, we cannot guarantee that our path is the shortest path from\\nthe source to C, so this path is not suitable to prune it.\\n643\\nConrad & Williams\\nA\\nB\\nC\\n-5\\n-4\\n1\\n(a)\\nLower\\ndomination\\nweighted graph\\nA\\nB\\nC\\n2\\n5\\n3\\n(b)\\nUpper\\ndomination\\nweighted graph\\nA\\n0\\nB\\n-5\\nC\\n-4\\n(c) Lower domination pre-\\ndecessor graph\\nA\\n0\\nB\\n2\\nC\\n5\\n(d) Upper domination pre-\\ndecessor graph\\nFigure 7.5: Simple edge domination examples.\\nExample 7.13 To demonstrate the application of these ideas, reconsider Figure 7.4a. Ex-\\nample 7.6 gave the possible paths in the graph.\\nThe ﬁrst path is A → B with path\\nenvironment {x = 1, y = 1}. After reaching B, the minimal vertex weight is 2, but there\\nare no extension of this path, so nothing can be pruned. In general, the ﬁrst step from the\\nsource vertex cannot be pruned.\\nThe next step of the traversal reaches C with environment {x = 1}, and the minimal\\npath length is 1. C cannot be pruned. This path cannot be extended to B, but it does\\nhave an extension to D, using the vertex weight ((6, C), {x = 1, y = 1}). This path length\\nis strictly longer than the path length of 1, and the environment of the value is equal to the\\nenvironment of the path, so we add it to the pruned list.\\nNext consider path A → C under environment {} and path length 3. The extension\\nto B using ((7, C), {y = 1}) is also prunable. Note that this path could never be used to\\nprune the shorter path length ((2, A), {x = 1, y = 1}) because {x = 1, y = 1} ̸= {y = 1}.\\nLikewise, the extension to D prunes ((8, C), {y = 1}).\\nCollecting the non-pruned edges means that the algorithm adds edge (A, B) to the\\noutput dispatchable graph with weight W(A, B) = {(2, {x = 1, y = 1})}, and adds edge\\n(A, C) with weight W(A, C) = {(1, {x = 1}), (3, {})}. We drop inﬁnite weights, allowing\\nthem to be implicitly speciﬁed, and both ﬁnite weights of D were pruned, so it does not\\nadd the edge (A, D) at all.\\n□\\n644\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nEssentially, the pruning algorithm has the same structure as the unlabeled fast compi-\\nlation algorithm. The major diﬀerence is that the values to prune have environments and\\nthat paths in the predecessor graph only exist under particular environments. Thus, the\\npruning step must satisfy the pruning requirement with the identical environment to prune\\na labeled value.\\n7.5 Mutual Dominance and Rigid Components\\nTsamardinos et al. (1998) showed that rigid components in the distance graph of an STN\\ncreate mutual dominance, where two edges (A, B) and (A, C) are each used as evidence\\nto prune the other, and both are incorrectly removed when the algorithm of the previous\\nsection is applied. The correct solution is to perform either pruning, but not both. Unfor-\\ntunately, it is diﬃcult to test for mutual dominance during the SSSP pruning step. Instead,\\nthey present a pre-processing step that identiﬁes the rigid components, updates the output\\ndispatchable graph accordingly, and alters the input weighted distance graph to remove the\\nrigid components entirely, before beginning the process of computing edges in the dispatch-\\nable form through repeated SSSPs. This leaves a problem with no mutually dominated\\nedges, and thus the pruning step can prune all edges found to be dominated without any\\nrisk of encountering this problem. This section adapts Tsamardinos et al.’s approach by\\nidentifying labeled rigid components and uses a labeled analogue of the alteration process.\\nExample 7.14 Consider the predecessor graph fragment in Figure 7.6. There is a path\\nA → B → C, with d[B] = 1 and d[C] = 1, so that d[B] ≤ d[C]. Thus, the edge (A, C)\\ncan be pruned. Likewise, there is a path A → C → B that allows us to prune (A, B) from\\nthe dispatchable graph. Then the result is that A is not constrained to the rest of the\\nevents, which would allow the dispatcher to select an inconsistent schedule. These edges\\nare mutually dominant. Our algorithm is able to prune both edges because there is a path\\nB → C and C → B, if either did not exist, only one pruning would take place.\\n□\\nA\\n0\\nB\\n1\\nC\\n1\\nFigure 7.6: A predecessor graph, (A, B) and (A, C) are mutually dominant.\\nTsamardinos et al. (1998) show that mutual dominance occurs if and only if there are\\nrigid components in the problem, which are collections of events that must be scheduled\\nwith a ﬁxed diﬀerence in their execution times. In Figure 7.6, B and C are constrained to\\noccur at the same time, so they are rigidly connected. Rigidly connected components in a\\n645\\nConrad & Williams\\nconsistent distance graph coincide with strongly-connected components in the predecessor\\ngraph for a single source shortest path for arbitrary connected vertices. Thus, they show\\nthat we can identify all instances of possible mutual dominance by ﬁnding all the strongly-\\nconnected components of the predecessor graphs, and then remove them.\\nIntuitively, we might understand the issue as the input problem having less degrees of\\nfreedom than it appears to have. If two events are actually constrained to occur at the same\\ninstant, then the dispatcher has one decision, not two. When compiling, each decision looks\\nredundant to the other, leading to incorrect prunings, as in Example 7.14. The solution\\nis to remove the redundancy by replacing the entire rigid component with a single vertex,\\ncalled the leader. The prior work actually removes all events other than the leader with a\\npreprocessing step, contracting the rigid component to a single vertex. Since we are working\\nwith labeled distance graphs, where rigid components may exist only in some environments,\\nwe must replicate this eﬀect indirectly, in two steps: provide information in the compiled\\nform so the dispatcher can respect the rigid component, and alter the input graph so that\\nunder conditions where the rigid component exists, the compilation algorithm only reasons\\nover the leader.\\nThe ﬁrst step of preprocessing is to identify the rigid components. Arbitrary graphs\\nmight not be connected, so an SSSP from any arbitrary vertex may not reach all the\\nvertices, and thus cannot ﬁnd rigid components in the disconnected subgraph. A standard\\nstrategy, as in the Johnson’s algorithm preprocessing step, is to introduce an extra vertex\\nfor the preprocessing step, connected to every vertex with a zero weight edge. This extra\\nvertex is connected to every vertex, which makes it an ideal source to search from, even for\\ndisconnected graphs. Furthermore, the extra vertex does not alter the rigid components in\\nthe predecessor graph. In the labeled case, the equivalent approach is to introduce a new\\nvertex connected to every other vertex with an edge with weight (0, {}), so that the zero\\nweight edge applies under every environment. Then, the SSSP is computed with the added\\nvertex as the source. Finally, the algorithm searches the predecessor graph for cycles. The\\nsimplest, if not necessarily the most eﬃcient, method in the labeled case is simply to use\\nour knowledge about ﬁnding paths in the predecessor graph to search for loops.\\nAs we might expect, rigid components in Labeled STNs are given labels, since they may\\nonly exist under some environments. We need to ﬁnd the maximal rigid components with\\nminimal environments. Thus, if there is a rigid component {A, B, C}, we do not separately\\nidentify {A, B} as a rigid component. For every vertex in V , run a depth ﬁrst search for a\\npath back to the vertex. If found, the vertices on the path are a rigid component, and the\\npath environment is the environment of the rigid component. Note that a valid path may\\nvisit any vertex up to twice, as in the example below. As the search ﬁnds rigid components,\\nit maintains a list of the maximally sized rigid components with minimal environments.\\nExample 7.15 Figure 7.7 shows a small labeled distance graph with vertices {A, B, C}.\\nRecall that having an opposing pair of edges, one with the negative weight of the other,\\nimplies a rigid component, so this problem obviously has some.\\nTo apply our technique, the algorithm ﬁrst adds another vertex X, and connects it\\nto each vertex with an edge with weight (0, {}). Figure 7.7b also shows the predecessor\\ngraph computed after this has been done.\\nSecond, the algorithm searches for loops in\\nthe graph.\\nIt begins with A: the path A → B → A with environment {x = 1} is a\\nloop, as is A → B → C → B → A with environment {x = 1, y = 1}.\\nThese paths\\n646\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nimply rigid components {A, B} with environment {x = 1} and {A, B, C} with environment\\n{x = 1, y = 1}, respectively. Note that the second path visits B twice. From B, it ﬁnd\\npaths B → A → B with environment {x = 1}, which is equivalent to the one found from\\nA, and B → C → B with environment {y = 1}, which is a new rigid component. The\\npaths from C re-derive the same rigid components. Thus, there are three maximal rigid\\ncomponents, {A, B} with environment {x = 1}, {B, C} with environment {y = 1}, and\\n{A, B, C} with environment {x = 1, y = 1}.\\nIf the two edges between B and C had the environment {x = 1} instead of {y = 1},\\nthen there would only be a single maximal rigid component {A, B, C} with environment\\n{x = 1}. The smaller components, such as {A, B} with {x = 1}, would be non-maximal\\nand not needed.\\n□\\nA\\nB\\nC\\n{(3, {x = 1})}\\n{(−3, {x = 1})}\\n{(5, {y = 1})}\\n{(−5, {y = 1})}\\n(a) Labeled distance graph\\n{((∞, nil), {})}\\nX\\nA\\nB\\nC\\n{((−3, B), {x = 1}),\\n((0, X), {})}\\n{((−5, C), {y = 1}),\\n((0, A), {x = 1}),\\n((0, X), {})}\\n{((0, B), {y = 1}),\\n((∞, nil), {})}\\n(b) Predecessor graph of the added vertex X.\\nFigure 7.7: Identifying rigid components in labeled distance graphs.\\nAfter completing the search for rigid components, we must process them.\\nThe ﬁrst\\nprocessing step is to identify a leader, the ﬁrst vertex to occur, which we will use to represent\\nthe entire rigid component. The leader is the vertex with the lowest distance from the\\nadded vertex, queried under the environment of the rigid component. Ties may be broken\\narbitrarily.\\nThe second processing step is to update the dispatchable form accordingly. Within the\\nrigid component, the events are rigidly bound together and the leader is executed before\\nany other event of the rigid component. Thus, in the dispatchable form, we constrain the\\nnon-leader events to occur the correct, ﬁxed amount of time after the leader. Thus, if A\\nis the leader of the rigid component, and B is some other vertex of the rigid component\\nwith environment e, the the output graph should have the labeled edge weight (d[B](e) −\\nd[A](e), e) on edge (A, B) and (d[A](e) − d[B](e), e) on edge (B, A).\\nIf any events are\\nrigidly connected in a way such that they will be executed at the same time, the dispatcher\\n647\\nConrad & Williams\\nneeds them listed explicitly, because the execution window and enablement tests alone do\\nnot guarantee correct execution of such plans. During this step we may identify maximal\\ngroups of events that are constrained to occur at the same time as the leader, or the same\\nﬁxed duration after the leader. For example, if A is the leader and both B and C follow A\\nby exactly three time units, then B and C are constrained to occur at the same time. As\\nusual, these zero-related vertices are recorded with a label, which is the same environment\\nas the rigid component.\\nThird, we need to alter the labeled distance graph so that the rigid component no longer\\nappears to exist, but is instead totally represented by the leader vertex. The algorithm\\nbegins with edges that are interior to the rigid component. If (A, B) are both vertices in a\\nrigid component with environment e, then the algorithm prunes W(A, B) and W(B, A) of\\nany environments subsumed by e. If (d, ed) ∈ W(A, B), then if e subsumes ed, the algorithm\\nremoves the value pair. If e does not subsume ed, then replace (d, ed) with labeled values\\n(d, e′\\nd) where e′\\nd avoids e, which may require multiple new values. Repeat for W(B, A).\\nFinally, we need to adjust edges that enter or leave the rigid component to ensure the\\ninput to the compiler has no cycles in any predecessor graphs. The idea is to move these\\nedges to the leader under the environments of the rigid component, and to remove them\\nfrom the non-leader. Assume A is the leader of a rigid component with environment e, B\\nis another vertex of the rigid component, and C is a vertex not in the rigid component. For\\nevery labeled value (d, ed) ∈ w(B, C) for some vertex, the algorithm puts (d + d[B](e) −\\nd[C](e), ed ∪ e) in W(A, C). Next, it replaces (d, ed) ∈ W(B, C) with values (d, e′\\nd) for each\\npossible e′\\nd that is a union of ed and the constituent kernels of e. The algorithm repeats\\nfor W(C, B), putting (d + d[C](e) − d[B](e), ed ∪ e) in W(C, A), and replacing (d, ed) with\\n(d′, e′\\nd) avoiding e. Unfortunately, this strategy leads to duplication that grows linearly with\\nthe domain size of the choice variables. In practice, though, many problems do not have\\nany rigid components, or they are limited and the penalty is minor. Our results show some\\noutliers that may be associated with this growth, but most problems are not aﬀected.\\nExample 7.16 As usual, let x, y ∈ {1, 2} be choice variables. Figure 7.8a depicts an input\\nlabeled distance graph fragment. From the opposing edge weights 1 and −1, we can easily\\nidentify that {A, B} is a rigid component under environment {x = 1}. We need to follow\\nthe above procedure to process it.\\nFirst, A is always scheduled before B, and is therefore the leader. If we ran the SSSP on\\nan added vertex, we would ﬁnd that d[A]({x = 1}) = −1 and d[B]({x = 1}) = 0, proving\\nthis assertion. Since d[B]({x = 1}) − d[A]({x = 1}) = 1, event B follows A by one time\\nunit. Therefore, edges are inserted into the output dispatchable form shown in Figure 7.8c\\nthat enforce this delay.\\nNext, we alter the input graph to remove the rigid component, beginning with edges\\nbetween the rigid component’s vertices. Edge (B, A) has one weight on it, which is subsumed\\nby the environment of the rigid component, so it only exists when the rigid component does,\\nand thus has already been handled, and is pruned. Likewise, the weight (1, {}) ∈ W(A, B)\\nis pruned.\\nOn the other hand, the weight (2, {x = 2}) ∈ W(A, B) already avoids the\\nenvironment of the rigid component, so is left unchanged.\\nContinue with edges that enter or leave the rigid component. (A, C) uses the leader\\nvertex, so does not directly require any modiﬁcation.\\nEdge (B, C) needs to be moved,\\ngiving a new weight (4 + 1, {x = 1}) on W(A, C), which is already present, and requires\\n648\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nno modiﬁcation. Since its environment is subsumed by the rigid component environment,\\n(4, {x = 1}) is removed from w(B, C). Edge (C, B) also needs to be moved. It leads to a\\nnew value (2 − 1, {y = 1} ∪ {x = 1}) = (1, {x = 1, y = 1}) ∈ W(C, A). Since {y = 1} is not\\nsubsumed by the rigid component’s environment, we must modify the value on (C, B) so\\nthat it avoids the environment. Avoiding {x = 1} means that {x = 2}, so we modify the\\nvalue (2, {y = 1}) → (2, {x = 2, y = 1}). These modiﬁcations completely remove the rigid\\ncomponent and provide the dispatcher with suﬃcient information to correctly execute the\\nrigid components we have removed.\\n□\\nAfter these modiﬁcations the labeled distance graph is guaranteed not to have any cycles\\nin the predecessor graphs, and therefore has no rigid components. Thus the rest of the fast\\nalgorithm given in this section correctly compiles it to dispatchable form. We adjusted\\nthe unlabeled algorithm to search for labeled rigid components. Additionally, the unlabeled\\nalgorithm removes the non-leader vertices of rigid components, moving and modifying edges\\nas necessary enforce the original input constraints. Since those vertices may be needed under\\nenvironments where the rigid component does not exist, Drake instead modiﬁes the process\\nof moving edges to replicate the eﬀect of removing the non-leader vertices.\\nDrake’s compilation algorithm is designed to use labeling concepts to compute a compact\\nversion of the dispatchable form of plans with choice. The structure of this algorithm is\\nsimilar to the unlabeled version, but a number of modiﬁcations are made within each step to\\nreason about environments. This section completes the presentation of Drake’s algorithms.\\n8. Results\\nFinally, we explore Drake’s performance, both from a theoretical standpoint and experi-\\nmentally. Our analysis gives some justiﬁcation for why we expect Drake’s representation to\\nbe compact, and our experimental results give evidence that Drake performs as intended.\\n8.1 Theoretical Results\\nWe give a brief characterization of the analytical worst case performance of Drake’s algo-\\nrithms. The direct enumeration of STNs, as in Tsamardinos et al.’s (2001) work, uses one\\nSTN for each consistent STN. If there are n choices with d options each, and v vertices,\\nand we assume that the compiled sparse graphs are of size O\\n�\\nv log v\\n�\\n, then the compiled\\nsize is O\\n�\\nndv log v\\n�\\n. In contrast, Drake does not store the component STNs independently,\\nbut only stores the distinct values, so the size is O\\n�\\nkv log v\\n�\\n, where k ≤ nd. In the worst\\ncase, where every single component STN is completely diﬀerent, there are no similarities\\nbetween choices to exploit, hence Drake’s compiled representation is the same size, but is\\nnever worse, up to a constant.\\nThere is a strong parallel to the existing theory about the tree width of general constraint\\nsatisfaction problems. Dechter and Mateescu (2007) explain that while a general constraint\\nsatisfaction problem with n variables of domain size d can in general only be solved in O\\n�\\nnd�\\nsteps, many problems have more structure. The tree width, n∗ ≤ n of the problem represents\\nthe number of variables that eﬀectively interact, so that the search can be completed in\\nO\\n�\\n(n∗)d�\\ntime. Similarly, in Labeled STNs, the choice variables may not interact fully, and\\nthus there is an eﬀective number of choice variables in the problem that is often smaller than\\n649\\nConrad & Williams\\nA\\nB\\nC\\n{(1, {x = 1}), (2, {x = 2})}\\n{(−1, {x = 1})}\\n{(5, {x = 1})}\\n{(4, {x = 1})}\\n{(2, {y = 1})}\\n(a) Input labeled distance graph\\nA\\nB\\nC\\n{(2, {x = 2})}\\n{(5, {x = 1})}\\n{(2, {x = 2, y = 1})}\\n{(1, {x = 1, y = 1})}\\n(b) Contracted labeled distance graph\\nA\\nB\\nC\\n{(1, {x = 1})}\\n{(−1, {x = 1})}\\n(c) Dispatchable form of rigid component\\nFigure 7.8: An example of processing a rigid component from a Labeled Distance Graph.\\nthe total number. In this notation, Drake’s compiled problems have size O\\n�\\n(n∗)dn log v\\n�\\n.\\nThe smaller base of the exponent can lead to signiﬁcant savings.\\n650\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nThe compile time and run-time latency are more diﬃcult to characterize, however,\\nbecause the overhead of the labeling operations also grows with n∗. Therefore, we do not\\nattempt an analytic analysis.\\n8.2 Experimental Results\\nThis section presents an experimental validation of Drake’s compilation and dispatch algo-\\nrithms on randomly generated, structured problems. First, we develop a suite of random\\nstructured Labeled STNs, derived from Stedl’s (2004) problem generator. Then we compile\\nand dispatch the suites of problems twice, once with Drake and once by explicitly enumer-\\nating all component STNs, following techniques developed by Tsamardinos et al. (2001).\\nFinally, we compare the compiled size of the problems, the compilation time, and the exe-\\ncution latency. Throughout this section, our plots use the number of consistent component\\nSTNs as the horizontal axis, because it appears to correlate well with the eﬀective diﬃculty\\nof the problem. For each of the metrics, we provide the results from the two methods side\\nby side in one plot, and show the ratio of performance in another one, to allow point-wise\\ncomparison of the diﬀerence in performance between identical problems. The problems are\\nconstructed from 2-11 binary choices or 2-7 ternary choices. There are 100 problems at\\neach of these sizes; the problems range from 4 consistent component STNs up to about\\n2000. The number of events ranges from 4 to about 22 as the number of component STNs\\nincrease to keep a consistent ratio of constraints to events.\\nThis comparison is performed with a Lisp implementation, run on a 2.66 GHz machine\\nwith 4 Gb of memory. There are some performance related implementation details we have\\nomitted in prior sections. For example, labeled value sets are stored as ordered lists to\\nreduce insertion and query time. Additionally, we found that memoization of subsumption\\nand union operations dramatically improved performance. The implementation aggressively\\nprunes values with inconsistent environments to avoid any unnecessary reasoning. The STN\\ncompiler and dispatcher exercise the same code as Drake to support a fair comparison, and\\npays a small overhead in execution speed.\\nThe ﬁrst metric of comparison is the size of the dispatchable form of the random prob-\\nlems. We computed this by serializing the graph representations to strings. Since Drake’s\\ncompilation algorithm is derived from the fast STN compiler, the maximum memory foot-\\nprint for both compilation and dispatch is no more than about double these numbers.\\nSince we designed Drake with this metric in mind, we expect the improvement to be clear\\nand signiﬁcant, and this is what Figure 8.1 shows. In the ratio plot, the value is the mul-\\ntiple of improvement of Drake over STN enumeration. Except for small problems, Drake’s\\nmemory performance is superior, and the improvement ranges up to around a 700 times\\nsmaller memory footprint for the largest problems. The STN enumeration Tsamardinos et\\nal. (2001) develops uses up to around 2 MB of storage, and although that is insigniﬁcant\\nfor a modern desktop, it is often signiﬁcant for embedded hardware, especially if the system\\nmust store a library of compiled plans. In contrast, Drake’s memory footprint is 1-10 kB for\\nmost cases, which is trivial, even in large numbers, for any hardware. More than half the\\nworse performing examples, those that do not ﬁt in the main band of results, correspond\\nto ternary choices. We believe this likely corresponds to the growth caused by avoiding\\nenvironments when handling complex or overlapping rigid components in these problems.\\n651\\nConrad & Williams\\n10\\n0\\n10\\n1\\n10\\n2\\n10\\n3\\n10\\n4\\n10\\n−1\\n10\\n0\\n10\\n1\\n10\\n2\\n10\\n3\\n10\\n4\\nNumber of Consistent Component STNs\\nCompiled Size (kB)  \\n \\n \\nDrake\\nSTN Enumeration\\n(a) The size of the dispatchable labeled distance graphs.\\n10\\n0\\n10\\n1\\n10\\n2\\n10\\n3\\n10\\n4\\n10\\n−1\\n10\\n0\\n10\\n1\\n10\\n2\\n10\\n3\\nNumber of Consistent Component STNs\\nCompiled Size Ratio STN/Labeled STN     \\n(b) The ratio of the size of the compiled STN enumeration size to the size of Drake’s labeled distance graph.\\nFigure 8.1: The size of the dispatchable form of the random problems as a function of the\\nnumber of component STNs.\\n652\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nThe second metric is the time required to compile the random problems, shown in Figure\\n8.2. Often, Drake’s compilation times are much better, but they are highly variable. For\\na number of the largest problems, Drake is up to about 1000 times faster. However, some\\nproblems exhibit little improvement, and a few are up to 100 times slower for the worst\\ncases. Most problems take less than about ten minutes for both methods, but Drake takes\\na few hours on ﬁve of the largest problems. We expect higher variability for the run-time\\nbecause the compilation algorithms loop repeatedly over the labeled graph, exacerbating\\nthe variability shown in the compiled size. Thus, it is not surprising that on problems whose\\ndispatchable form is not compact, the run-time suﬀers.\\nThe ﬁnal metric is the run-time latency incurred by the algorithms, shown in Figure\\n8.3. Drake’s reported latency is the maximum latency for a single decision making period\\nduring a single execution of the entire problem. The STN enumeration latency reported\\nis the time required to identify, execute, and propagate the ﬁrst event in each consistent,\\ncomponent STN. While these metrics are not identical, they are quite comparable. The\\nvalues reported at 10−3 seconds were reported as zero by Lisp’s timing features, and we\\ninﬂate them to ﬁt on a log scale.\\nAlthough diﬀerences in compilation time are interesting, increases in the run-time la-\\ntency are far more critical to Drake’s applicability in the real world.\\nFortunately, the\\nsituation looks favorable. Both systems execute all but the largest problems in under a\\ntenth of a second, and most small and moderate sized problems in around 10 milliseconds.\\nAlthough Drake is slower on a reasonable fraction of the problems, the margin is fairly low;\\nnote that the cluster of values at a ratio of just less than 10−1 corresponds to the jump from\\neﬀectively zero to about 10 milliseconds. We do not know the real ratio, but these points\\ncreate a visible cluster at 10−1 which may or may not be misleading. Instead, we focus on\\nlarger problems where both methods are measurable, and where Drake generally performs\\nquite well. Again, a handful of problems are outliers, taking much more time, up to tens\\nof seconds, which would not be acceptable for most applications. We conclude that Drake\\nwill perform well on embedded systems for many real world problems, in terms of memory\\nusage and latency.\\nOverall, these results are what Drake was designed to achieve. Using a compact represen-\\ntation provides a smaller memory footprint. Sometimes, exploiting the similarity between\\nchoices makes reasoning fast, and other times it imposes an extra computational burden to\\ntease out the similarities, or lack thereof. Enumerating the STNs directly has quite pre-\\ndictable costs, both in time and space, and Drake is far more variable, depending on the\\nprecise nature of the problem. While we ﬁnd these results quite promising, we must caution\\nthat applications to particular problems could be better or worse, depending on factors we\\ndo not know how to easily characterize. This is well illustrated by the few problems that\\nwere outliers in all three metrics. It may be useful in future work to further investigate the\\nsources of variability in Drake’s performance.\\nAs a practical note, we tightly regulated the number of events to focus our investigation\\non the scaling with the number of choices. However, both algorithms should scale gracefully,\\nand with similar increases in space and time costs, to plans with more events. Overall,\\nDrake appears to provide a noticeably lower memory footprint for dispatching problems\\nwith discrete choices than the direct enumeration strategy of Tsamardinos et al. (2001),\\nwhile only suﬀering from a mild increase in run-time latency.\\n653\\nConrad & Williams\\n10\\n0\\n10\\n1\\n10\\n2\\n10\\n3\\n10\\n4\\n10\\n−2\\n10\\n0\\n10\\n2\\n10\\n4\\n10\\n6\\nNumber of Consistent Component STNs\\nCompile Time (sec)\\n \\n \\nDrake\\nSTN Enumeration\\n(a) The compile time for random problems.\\n10\\n0\\n10\\n1\\n10\\n2\\n10\\n3\\n10\\n4\\n10\\n−2\\n10\\n0\\n10\\n2\\n10\\n4\\nNumber of Consistent Component STNs\\nCompile Time Ratio, STN/Labeled STN\\n(b) The ratio of the compile time of STN enumeration to Drake’s compile time.\\nFigure 8.2: The compile time of random problems as a function of the number of component\\nSTNs.\\n654\\nDrake: An Efficient Executive for Temporal Plans with Choice\\n10\\n0\\n10\\n1\\n10\\n2\\n10\\n3\\n10\\n4\\n10\\n−3\\n10\\n−2\\n10\\n−1\\n10\\n0\\n10\\n1\\nNumber of Consistent Component STNs\\nExecution Latency (sec)\\n \\n \\nDrake\\nSTN Enumeration\\n(a) The execution latency.\\n10\\n0\\n10\\n1\\n10\\n2\\n10\\n3\\n10\\n4\\n10\\n−2\\n10\\n−1\\n10\\n0\\n10\\n1\\n10\\n2\\nNumber of Consistent Component STNs\\nExecution Latency Ratio STN/Labeled STN\\n(b) The ratio of the execution latency for STN enumeration to Drake’s execution latency.\\nFigure 8.3: The execution latency of random problems as a function of the number of com-\\nponent STNs.\\n655\\nConrad & Williams\\n9. Summary\\nThis work presents Drake, a compact, ﬂexible executive for plans with choice. Drake takes\\ninput plans with temporal ﬂexibility and discrete choices, such as Labeled STNs or DTNs,\\nand selects the execution times and makes discrete decisions at run-time (Dechter et al.,\\n1991). Choices substantially improve the expressiveness of the tasks that executives can\\nperform, and improve the robustness of the resulting executions. Prior execution approaches\\ntypically impose signiﬁcant memory requirements or introduce substantial latency during\\nexecution. Our goal in developing Drake is to develop a dispatching executive with a lower\\nmemory footprint.\\nBuilding upon the concept of labels employed by the ATMS to compactly encode all\\nthe consequences of a set of alternative choices, Drake introduces a new compact encoding,\\ncalled labeled distance graphs, to encode and eﬃciently reason over discrete choices, and\\nwe introduce a corresponding maintenance system (de Kleer, 1986). Our adaptation of the\\nATMS labeling scheme focuses on only maintaining non-dominated constraints, which allows\\nDrake to exploit the structure of temporal reasoning, cast as a shortest path problem on a\\ndistance graph, to provide a compact representation. Furthermore, modifying the existing\\nunlabeled algorithms to account for labels does not change the overall structure of the\\nalgorithms.\\nDrake’s compilation algorithm successfully compresses the dispatchable solution by over\\ntwo orders of magnitude relative to Tsamardinos, Pollack, and Ganchev’s (2001) prior work,\\noften reducing the compilation time, and typically introducing only a modest increase in\\nexecution latency. Thus, we believe that Drake successfully realizes our initial goals. Within\\nour experiments, compilation typically takes less than ten minutes, but on occasion takes\\nhours. Although time consuming in the later case, this is still acceptable, since compilation\\ncan be performed oﬀ-line, when a task is ﬁrst deﬁned. To summarize, Drake’s Labeled\\nSTNs and labeled distance graphs enable an executive that strikes a useful balance between\\nlatency and memory consumed, which is appropriate for real world applications. Drake’s\\nlabeling scheme also provides the opportunity to extend a wide range of graph algorithms\\nto reason about and represent choice eﬃciently.\\nAcknowledgments\\nThe authors would like to thank Julie Shah and David Wang for many helpful ideas and\\ndiscussions, and the reviewers for their insightful comments. Patrick Conrad was funded\\nduring this work by a Department of Defense NDSEG Fellowship.\\nReferences\\nBlock, S., Wehowsky, A., & Williams, B. (2006). Robust execution of contingent, temporally\\nﬂexible plans. In Proceedings of the 21st National Conference on Artiﬁcial Intelligence,\\npp. 802–808.\\nCombi, C., & Posenato, R. (2009).\\nControllability in temporal conceptual workﬂow\\nschemata. In Dayal, U., Eder, J., Koehler, J., & Reijers, H. (Eds.), Business Process\\n656\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nManagement, Vol. 5701 of Lecture Notes in Computer Science, pp. 64–79. Springer\\nBerlin / Heidelberg.\\nCombi, C., & Posenato, R. (2010).\\nTowards temporal controllabilities for workﬂow\\nschemata.\\nIn Proceedings of the 17th International Symposium on Temporal Rep-\\nresentation and Reasoning, pp. 129–136. IEEE.\\nConrad, P. R. (2010). Flexible execution of plans with choice and uncertainty. Master’s\\nthesis, Massachusetts Institute of Technology.\\nConrad, P. R., Shah, J. A., & Williams, B. C. (2009). Flexible execution of plans with choice.\\nIn Proceedings of the Nineteenth International Conference on Automated Planning and\\nScheduling (ICAPS-09). AAAI Press.\\nCormen, T., Leiserson, C., Rivest, R., & Stein, C. (2001). Introduction to algorithms (Second\\nedition). The MIT Press.\\nde Kleer, J. (1986). An assumption-based TMS. Artiﬁcial intelligence, 28(2), 127–162.\\nDechter, R., & Mateescu, R. (2007). AND/OR search spaces for graphical models. Artiﬁcial\\nIntelligence, 171(2-3), 73–106.\\nDechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artiﬁcial Intelli-\\ngence, 49(1-3), 61 – 95.\\nDoyle, J. (1979). A truth maintenance system* 1. Artiﬁcial Intelligence, 12(3), 231–272.\\nEﬃnger, R. (2006).\\nOptimal Temporal Planning at Reactive Time Scales via Dynamic\\nBacktracking Branch and Bound. Master’s thesis, Massachusetts Institute of Tech-\\nnology.\\nEn, N., & Srensson, N. (2004). An extensible sat-solver. In Giunchiglia, E., & Tacchella, A.\\n(Eds.), Theory and Applications of Satisﬁability Testing, Vol. 2919 of Lecture Notes\\nin Computer Science, pp. 333–336. Springer Berlin / Heidelberg.\\nGoldstone, D. (1991). Controlling inequality reasoning in a TMS-based analog diagnosis\\nsystem. In Proceedings of the Ninth National Conference on Artiﬁcial Intelligence,\\npp. 512–517.\\nHiatt, L., Zimmerman, T., Smith, S., & Simmons, R. (2009).\\nStrengthening schedules\\nthrough uncertainty analysis. In Proceedings of the International Joint Conference on\\nArtiﬁcial Intelligence, Vol. 2, pp. 5–3.\\nHunsberger, L. (2009). Fixing the semantics for dynamic controllability and providing a\\nmore practical characterization of dynamic execution strategies. In Proceedings of\\nthe 16th International Symposium on Temporal Representation and Reasoning, pp.\\n155–162. IEEE.\\nHunsberger, L. (2010). A Fast Incremental Algorithm for Managing the Execution of Dy-\\nnamically Controllable Temporal Networks. In Proceedings of the 17th International\\nSymposium on Temporal Representation and Reasoning, pp. 121–128. IEEE.\\nKhatib, L., Morris, P., Morris, R., & Rossi, F. (2001).\\nTemporal constraint reasoning\\nwith preferences. In Proceedings of the International Joint Conference on Artiﬁcial\\nIntelligence, Vol. 1, pp. 322–327.\\n657\\nConrad & Williams\\nKim, P., Williams, B. C., & Abramson, M. (2001). Executing reactive, model-based pro-\\ngrams through graph-based temporal planning. In Proceedings of the International\\nJoint Conference on Artiﬁcial Intelligence, Vol. 17, pp. 487–493.\\nMcDermott, D. (1983). Contexts and data dependencies: A synthesis. Pattern Analysis and\\nMachine Intelligence, IEEE Transactions on, PAMI-5(3), 237–246.\\nMorris, P. (2006). A structural characterization of temporal dynamic controllability. Prin-\\nciples and Practice of Constraint Programming, 4204, 375–389.\\nMorris, P., Muscettola, N., & Vidal, T. (2001). Dynamic control of plans with temporal\\nuncertainty. In Proceedings of the International Joint Conference on Artiﬁcial Intel-\\nligence, Vol. 17, pp. 494–502.\\nMuscettola, N., Morris, P., & Tsamardinos, I. (1998). Reformulating temporal plans for\\neﬃcient execution. In Proceedings of the Principles of Knowledge Representation and\\nReasoning-International Conference, pp. 444–452.\\nPlanken, L., de Weerdt, M., & van der Krogt, R. (2008). P 3 C: A New Algorithm for the\\nSimple Temporal Problem. In Proceedings of the Eighteenth International Conference\\non Automated Planning and Scheduling (ICAPS-08), pp. 256–263. AAAI Press.\\nRossi, F., Venable, K., & Yorke-Smith, N. (2006). Uncertainty in soft temporal constraint\\nproblems: a general framework and controllability algorithms for the fuzzy case. Jour-\\nnal of Artiﬁcial Intelligence Research, 27(1), 617–674.\\nShah, J., Stedl, J., Williams, B., & Robertson, P. (2007). A fast incremental algorithm for\\nmaintaining dispatchability of partially controllable Plans. In Proceedings of the Sev-\\nenteenth International Conference on Automated Planning and Scheduling (ICAPS-\\n2007). AAAI Press.\\nShah, J. A., & Williams, B. C. (2008).\\nFast Dynamic Scheduling of Disjunctive Tem-\\nporal Constraint Networks through Incremental Compilation. In Proceedings of the\\nNineteenth International Conference on Automated Planning and Scheduling (ICAPS-\\n2008). AAAI Press.\\nShu, I.-h., Eﬃnger, R., & Williams, B. C. (2005). Enabling Fast Flexible Planning Through\\nIncremental Temporal Reasoning with Conﬂict Extraction.\\nIn Proceedings of the\\nFifteenth International Conference on Automated Planning and Scheduling (ICAPS-\\n05), pp. 252–261. AAAI Press.\\nSmith, S., Gallagher, A., & Zimmerman, T. (2007). Distributed management of ﬂexible\\ntimes schedules.\\nIn Proceedings of the 6th International Joint Conference on Au-\\ntonomous Agents and Multiagent Systems, pp. 1–8. ACM.\\nStallman, R., & Sussman, G. (1977). Forward reasoning and dependency-directed back-\\ntracking in a system for computer-aided circuit analysis* 1. Artiﬁcial Intelligence,\\n9(2), 135–196.\\nStedl, J. (2004). Managing temporal uncertainty under limited communication: a formal\\nmodel of tight and loose team coordination. Master’s thesis, Massachusetts Institute\\nof Technology.\\n658\\nDrake: An Efficient Executive for Temporal Plans with Choice\\nStergiou, K., & Koubarakis, M. (2000). Backtracking algorithms for disjunctions of temporal\\nconstraints. Artiﬁcial Intelligence, 120(1), 81–117.\\nTsamardinos, I. (2002). A probabilistic approach to robust execution of temporal plans\\nwith uncertainty. Methods and Applications of Artiﬁcial Intelligence, 2308, 751–751.\\nTsamardinos, I., Muscettola, N., & Morris, P. (1998).\\nFast transformation of temporal\\nplans for eﬃcient execution. In Proceedings of the Fifteenth National Conference on\\nArtiﬁcial Intelligence, pp. 254–261.\\nTsamardinos, I., Pollack, M., & Ganchev, P. (2001). Flexible dispatch of disjunctive plans.\\nIn 6th European Conference on Planning, pp. 417–422.\\nTsamardinos, I., Vidal, T., & Pollack, M. (2003). Ctp: A new constraint-based formalism\\nfor conditional, temporal planning. Constraints, 8(4), 365–388.\\nVenable, K., & Yorke-Smith, N. (2005). Disjunctive temporal planning with uncertainty.\\nIn Proceedings of the International Joint Conference on Artiﬁcial Intelligence, pp.\\n1721–22. Citeseer.\\nWilliams, B., & Ragno, R. (2007). Conﬂict-directed A* and its role in model-based embed-\\nded systems. Discrete Applied Mathematics, 155(12), 1562–1595.\\nWilliams, B. C., Ingham, M. D., Chung, S. H., & Elliott, P. H. (2003). Model-based pro-\\ngramming of intelligent embedded systems and robotic space explorers. Proceedings\\nof the IEEE: Special Issue on Modeling and Design of Embedded Software, 91(1),\\n212–237.\\nXu, L., & Choueiry, B. (2003). A new eﬀcient algorithm for solving the simple temporal\\nproblem. In Proceedings of the 10th International Symposium on Temporal Represen-\\ntation and Reasoning and Fourth International Conference on Temporal Logic, pp.\\n210–220.\\n659\\n', metadata={'Published': '2014-01-21', 'Title': 'Drake: An Efficient Executive for Temporal Plans with Choice', 'Authors': ['Patrick Raymond Conrad', ' Brian Williams'], 'Summary': '  This work presents Drake, a dynamic executive for temporal plans with choice.\\nDynamic plan execution strategies allow an autonomous agent to react quickly to\\nunfolding events, improving the robustness of the agent. Prior work developed\\nmethods for dynamically dispatching Simple Temporal Networks, and further\\nresearch enriched the expressiveness of the plans executives could handle,\\nincluding discrete choices, which are the focus of this work. However, in some\\napproaches to date, these additional choices induce significant storage or\\nlatency requirements to make flexible execution possible.\\n  Drake is designed to leverage the low latency made possible by a\\npreprocessing step called compilation, while avoiding high memory costs through\\na compact representation. We leverage the concepts of labels and environments,\\ntaken from prior work in Assumption-based Truth Maintenance Systems (ATMS), to\\nconcisely record the implications of the discrete choices, exploiting the\\nstructure of the plan to avoid redundant reasoning or storage. Our labeling and\\nmaintenance scheme, called the Labeled Value Set Maintenance System, is\\ndistinguished by its focus on properties fundamental to temporal problems, and,\\nmore generally, weighted graph algorithms. In particular, the maintenance\\nsystem focuses on maintaining a minimal representation of non-dominated\\nconstraints. We benchmark Drakes performance on random structured problems, and\\nfind that Drake reduces the size of the compiled representation by a factor of\\nover 500 for large problems, while incurring only a modest increase in run-time\\nlatency, compared to prior work in compiled executives for temporal plans with\\ndiscrete choices.\\n', 'paper_id': '1401.4606', 'journal_ref': 'Journal Of Artificial Intelligence Research, Volume 42, pages\\n  607-659, 2011', 'categories': 493837    cs.AI\n",
       "Name: categories, dtype: object, 'source': 'http://arxiv.org/abs/1401.4606'})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
